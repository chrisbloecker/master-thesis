\chapter{Hive}
In this chapter we will describe a concrete implementation of the algebraic model from \chpref{chp:algebraic_model} in Haskell. We will start off with the definition of the data structure and then take a closer look at the implementation of the interpreter that will take care its distribution in a distributed network. We will conclude this chapter with a collection of examples that will illustrate how to use our algebra to solve problems.

\section{Cloud Haskell}
We will base our implementation on Cloud Haskell \cite{Epstein:2011:THC:2034675.2034690}, a domain specific language for distributed programming in Haskell. Cloud Haskell is highly inspired by Erlang and uses message passing for communication between processes, there is no implicitly shared memory.

A Cloud Haskell process is a function that runs in the \texttt{Process} monad and can be spawned on a local or remote node. Processes can send messages to other processes if they have knowledge about their process identifier, which serves as an address.

While Erlang uses atoms as tags for messages, Cloud Haskell uses data types that need to be an instance of \texttt{Serializable}. \texttt{Serializable} itself is only a combination of both \texttt{Binary} and \texttt{Typeable}. \texttt{Binary} is neccessary to serialise a message into a \texttt{ByteString}, \texttt{Typeable} is used to identify the type of a message. This way, serialisation is made explicit, in contrast to Erlang where it is implicit \cite{Epstein:2011:THC:2034675.2034690}.

In Haskell, functions can only be executed, composed and passed as arguments, they cannot be serialised. However, this would be neccessary in order to send a function to a remote node and execute it there. Cloud Haskell avoids this problem by using a table of static code pointers, i.e. fully qualified top level names of functions that are known at compile time, to refer to functions by a name. For remote execution, a function's name will be put into a \texttt{Closure}, together with its serialised environment, and sent to a remote node where it will be deserialised and executed. A \texttt{Closure} is nothing more than just mentioned: a function together with its environment \cite{Epstein:2011:THC:2034675.2034690}.

After a \texttt{Closure} has been executed, the result will be serialised and sent back to the caller. However, in some cases, the type system cannot infer the serialisability of the result type and therefore additional information needs to be provided. For a type \texttt{a}, serialisation information can be provided with a value of type \texttt{Static (SerializableDict a)}. Essentially this is only an explicit type tag that enables the selection of the correct serialisation function for type \texttt{a}.

\section{The Hive algebra}
Our implementation is supposed to resemble the structure and expressiveness of the algebraic model given in \chpref{chp:algebraic_model}. Furthermore, we prefer to prevent the creation of erroneous processes rather than dealing with them when we execute a process. To achieve this, we will employ a generalised algebraic data type and leverage the power of Haskell's type system to create a model that will only allow for the creation of valid processes.

In the following \texttt{CH} is short for Cloud Haskell and will be the name for the qualified import of \texttt{Control.Distributed.Process}. Our data type \texttt{Process} for Hive processes will look like
\begin{lstlisting}[language=Haskell]
data Process a b where 
\end{lstlisting}
The type parameters \texttt{a} and \texttt{b} reflect the process' input and output types where \texttt{a} is the input type and \texttt{b} is the output type. For every data constructor of \texttt{Process}, we need to constrain the output type to have an instance of \texttt{Serializable} so the result can be serialised and sent back to the caller.

The Hive process algebra incorporates two data constructors for the creation of basic processes from Cloud Haskell processes, i.e. \texttt{Const} and \texttt{Simple}. In principle, it is possible to use an arbitraty \texttt{CH.Process} to create a Hive \texttt{Process}, but regarding possible transformations based on the laws introduced in \chpref{chp:laws}, we require them to be free of side-effects in order to preserve their semantics after transformation. Idempotent actions should not alter processes' semantics regardless of transformations. However, maintaining a consistent environment across multiple nodes is troublesome and not part of our goal, but leaves room for future work and improvements. A process should be referentially transparent, i.e. its behaviour should be fully determined by its input and repeated execution of the same process with the same input should always yield the same output.
\begin{lstlisting}[language=Haskell]
Const :: (Serializable b) 
      => CH.Static (SerializableDict b)
      -> CH.Closure (CH.Process b)
      -> Process a b
\end{lstlisting}
\texttt{Const} takes a \texttt{CH.Static (SerializableDict b)} and a \texttt{Closure (CH.Process b)}, i.e. a closure that contains a \texttt{CH.Process} that produces a value of type \texttt{b} when executed. Note that a \texttt{Process} constructed with \texttt{Const} does not take any other parameters and therefore will always produce the same, a constant, result, no matter what input value might be presented to it. 

\begin{lstlisting}[language=Haskell]
Simple :: (Serializable b) 
       => CH.Static (SerializableDict b)
       -> (a -> CH.Closure (CH.Process b))
       -> Process a b
\end{lstlisting}
\texttt{Simple} takes a \texttt{CH.Static (SerializableDict b)} and, instead of a closure, a closure generator \texttt{a $\to$ CH.Closure (CH.Process b)}. The closure generator will generate a closure of type \texttt{CH.Closure (CH.Process b)} when given a value of type \texttt{a}. This value of type \texttt{a} serves as the environment for the process inside the closure.

As mentioned, \texttt{Const} and \texttt{Simple} are the data constructors for the creation of Hive processes and represent the basic processes we saw in \chpref{chp:algebraic_model}. All other data constructors operate on existing Hive processes and resemble the algebraic operations from \chpref{chp:algebraic_model}, such as choice, parallel composition, sequential composition and \textsc{Kleene} star. However, as we will see, some of the data constructors will be more general and thus more expressive and flexible than the operators presented in \chpref{chp:algebraic_model}.

\begin{lstlisting}[language=Haskell]
Local :: (Serializable b) 
      => Process a b
      -> Process a b
\end{lstlisting}
\texttt{Local} simply wraps a Hive process without altering its behaviour or adding new data, the structure of \texttt{Local} is that of a decorator \cite{Gamma:1995:DPE:186897}. The purpose of \texttt{Local} is to give an indication to the process interpreter\footnote{We will have a closer look at the interpreter in \chpref{chp:implementation}. For now, the reader is invited to accept the existence of an interpreter that takes care of the distribution and execution of Hive process structures.} that the wrapped process should be executed locally. Typically, the reason for this arises from the expectation that the action of serialising a closure, sending it to a remote node, executing it there and obtaining the result is more expensive\footnote{Expensive in terms of the neccessary amount of time.} than executing the respective process locally. Since there is no general approach to estimate the neccessary amount of time to run a program \cite{}, we decide to equip the user with the \texttt{Local} data constructor and burden him with the obligation to make appropriate use of it.

\begin{lstlisting}[language=Haskell]
Choice :: (Serializable b)
       => c
       -> (a -> c -> d)
       -> (d -> Bool)
       -> Process a b
       -> Process a b
       -> Process a b
\end{lstlisting}
The \texttt{Choice} data constructor is used to build a process that makes a choice between two processes an represents the choice operator $\vee$. Unlike in the algebraic model from \chpref{chp:algebraic_model}, this choice is not made nondeterministically, but based on a predicate. \texttt{Choice} takes a value of type \texttt{c}, a function of type \texttt{a $\to$ c $\to$ d}, a predicate \texttt{d $\to$ Bool} and two processes of type \texttt{Process a b}. The interplay of the additional parameters will become clear in \chpref{chp:implementation}. For now, it will suffice to say that they enable process choice based on a static or dynamic value.

\begin{lstlisting}[language=Haskell]
Sequence :: (Serializable b, Serializable c)
         => Process a c
         -> Process c b
         -> Process a b
\end{lstlisting}
The \texttt{Sequence} data constructor takes two processes and composes them sequentially, it represents the sequence operator $\circ$. The result types of both processes need to have an instance of \texttt{Serializable} and the input type of the second process must correspond to the output type of the first process.

\begin{lstlisting}[language=Haskell]
Parallel :: (Serializable b, Serializable c, Serializable d)
         => Process a c
         -> Process a d
         -> Process (c, d) b
         -> Process a b
\end{lstlisting}
With the \texttt{Parallel} data constructor we can create a \texttt{Process} that runs two processes in parallel. As the signature of \texttt{Parallel} shows, our model for parallel composition of processes is more general than the one introduced in \chpref{chp:algebraic_model}. Where in the algebraic model from \chpref{chp:algebraic_model}, we can only combine two processes with the parallel combinator $|$ iff they have the same type signature and their result type forms a semigroup, we can use arbitrary processes for parallel composition in the \textsc{Hive} process algebra, however, they need to accept the same input type. \texttt{Parallel} takes two processes, with types \texttt{Process a c} and \texttt{Process a d}, that are to be composed in a parallel way. In addition to that, it takes a third process of type \texttt{Process (c, d) b}, and uses it to combine the results of the other two processes after they have finished their execution. The user may be well advised to consider wrapping the combinator process in a \texttt{Local} process since this may potentially be a cheap operation. Note that using an explicit process to combine the results is exactly what enables us to compose processes of different types in parallel. At the same time we eliminate the required structure of a semigroup on their result type.

\begin{lstlisting}[language=Haskell]
Multilel :: (Serializable b, Serializable c)
         => [Process a c]
         -> b
         -> Process (b, [c]) b
         -> Process a b
\end{lstlisting}
\texttt{Multilel} represents the generalisation of parallel composition of two processes to parallel composition of an arbitrary number of processes. It takes a list of processes of type \texttt{Process a c}, a value of type \texttt{b} and a process of type \texttt{Process (b, [c]) b}. The list of processes contains the processes that should be composed in parallel, the additional process together with the value of type \texttt{b} is used to fold the results of the processes together.

\begin{lstlisting}[language=Haskell]
Loop :: (Serializable b)
     => b
     -> c
     -> (c -> Bool)
     -> (b -> a)
     -> (a -> c -> c)
     -> Process a b
     -> Process a b
\end{lstlisting}
With the \texttt{Loop} data constructor, we can wrap a process for repeated execution, resembling the \textsc{Kleene} star. However, we provide a much more sophisticated and general version than the \texttt{Kleene} star does or than can be found in imperative programming languages. First, \texttt{Loop} takes a value of type \texttt{b}, which serves as a default output value in case the loop is executed exatly zero times. In imperative programming, this is the implicitly unchanged global state of the program. Then it takes a value of type \texttt{c}, a predicate of type \texttt{c $\to$ Bool} and a function of type \texttt{a $\to$ c $\to$ c}. The combination of these three controls the termination of the \texttt{Loop}'s execution. The function of type \texttt{b $\to$ a} is used to convert the result of the process of type \texttt{Process a b} back to a value of type \texttt{a}, so it can be fed into the next execution of the \texttt{Loop}.

\section{Implementation}
\label{chp:implementation}

\section{Examples}

\subsection{Hello world for interpreters}

\subsection{An ant system for the TSP}