\chapter{Hive}
In this chapter we will describe a concrete implementation of the algebraic model from \chpref{chp:algebraic_model} in Haskell. We will start off with the definition of the data structure and then take a closer look at the implementation of the interpreter that will take care its distribution in a distributed network. We will conclude this chapter with a collection of examples that will illustrate how to use our algebra to solve problems.

\section{Cloud Haskell}
We will base our implementation on Cloud Haskell \cite{Epstein:2011:THC:2034675.2034690}, a domain specific language for distributed programming in Haskell. Cloud Haskell is highly inspired by Erlang and uses message passing for communication between processes, there is no implicitly shared memory.

A Cloud Haskell process is a function that runs in the \texttt{Process} monad and can be spawned on a local or remote node. Processes can send messages to other processes if they have knowledge about their process identifier, which serves as an address.

While Erlang uses atoms as tags for messages, Cloud Haskell uses data types that need to be an instance of \texttt{Serializable}. \texttt{Serializable} itself is only a combination of both \texttt{Binary} and \texttt{Typeable}. \texttt{Binary} is neccessary to serialise a message into a \texttt{ByteString}, \texttt{Typeable} is used to identify the type of a message. This way, serialisation is made explicit, in contrast to Erlang where it is implicit \cite{Epstein:2011:THC:2034675.2034690}.

In Haskell, functions can only be executed, composed and passed as arguments, they cannot be serialised. However, this would be neccessary in order to send a function to a remote node and execute it there. Cloud Haskell avoids this problem by using a table of static code pointers, i.e. fully qualified top level names of functions that are known at compile time, to refer to functions by a name. For remote execution, a function's name will be put into a \texttt{Closure}, together with its serialised environment, and sent to a remote node where it will be deserialised and executed. A \texttt{Closure} is nothing more than just mentioned: a function together with its environment \cite{Epstein:2011:THC:2034675.2034690}.

After a \texttt{Closure} has been executed, the result will be serialised and sent back to the caller. However, in some cases, the type system cannot infer the serialisability of the result type and therefore additional information needs to be provided\footnote{Specifically, the problem is that the type constructors can be constrained with required type class instances for parameters. On deconstruction, this information is not available and therefore needs to be added explicitly again.}. For a type \texttt{a}, serialisation information can be provided with a value of type \texttt{Static (SerializableDict a)}. Essentially this is only an explicit type tag that enables the selection of the correct serialisation function for type \texttt{a}.

\section{The Hive process algebra}
Our implementation resembles the structure and expressiveness of the algebraic model given in \chpref{chp:algebraic_model}. Furthermore, we prefer to prevent the creation of erroneous processes rather than dealing with them when we execute a process. To achieve this, we will employ a generalised algebraic data type and leverage the power of Haskell's type system to create a model that will only allow for the creation of valid processes.

In the following \texttt{CH} is short for Cloud Haskell and will be the name for the qualified import of \texttt{Control.Distributed.Process}. Our data type \texttt{Process} for Hive processes will look like
\begin{lstlisting}[language=Haskell,caption=Data type for \textsc{Hive} processes.,numbers=left,frame=bt]
data Process a b where
\end{lstlisting}
The type parameters \texttt{a} and \texttt{b} reflect the process' input and output types where \texttt{a} is the input type and \texttt{b} is the output type. For every data constructor of \texttt{Process}, we need to constrain the output type to have an instance of \texttt{Serializable} so the result can be serialised and sent back to the caller. The \textsc{Hive} \texttt{Process} data type however, does not have an instance of \texttt{Serializable}. This means that \textsc{Hive} processes can \textbf{not} be serialized and sent over the network for remote execution. However, as we will see, basic processes contain a closure that can be serialized and sent over the network.

The Hive process algebra incorporates two data constructors for the creation of basic processes from Cloud Haskell processes, i.e. \texttt{Const} and \texttt{Simple}. In principle, it is possible to use an arbitrary \texttt{CH.Process} to create a Hive \texttt{Process}, but regarding possible transformations based on the laws introduced in \chpref{chp:laws}, we require them to be free of side-effects in order to preserve their semantics after transformation. Idempotent actions should not alter processes' semantics regardless of transformations. However, maintaining a consistent environment across multiple nodes is troublesome and not part of our goal, but leaves room for future work and improvements. A process should be referentially transparent, i.e. its behaviour should be fully determined by its input and repeated execution of the same process with the same input should always yield the same output.
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Const} type constructor.,numbers=left,frame=bt]
Const :: (Serializable b) 
      => CH.Static (SerializableDict b)
      -> CH.Closure (CH.Process b)
      -> Process a b
\end{lstlisting}
The \texttt{Const} data constructor takes a \texttt{CH.Static (SerializableDict b)} and a \texttt{Closure (CH.Process b)}, i.e. a closure that contains a \texttt{CH.Process} that produces a value of type \texttt{b} when executed. Note that a \texttt{Process} constructed with \texttt{Const} does not take any other parameters and therefore will always produce the same, i.e. a constant, result no matter what input value might be presented to it. 

\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Simple} type constructor.,numbers=left,frame=bt]
Simple :: (Serializable b) 
       => CH.Static (SerializableDict b)
       -> (a -> CH.Closure (CH.Process b))
       -> Process a b
\end{lstlisting}
\texttt{Simple} takes a \texttt{CH.Static (SerializableDict b)} and, instead of a closure, a closure generator \texttt{a $\to$ CH.Closure (CH.Process b)}. The closure generator will generate a closure of type \texttt{CH.Closure (CH.Process b)} when given a value of type \texttt{a}. This value of type \texttt{a} serves as the environment for the process inside the closure.

As mentioned, \texttt{Const} and \texttt{Simple} are the data constructors for the creation of Hive processes and represent the atomic processes we saw in \chpref{chp:algebraic_model}. All other data constructors operate on existing Hive processes and resemble the algebraic operations from \chpref{chp:algebraic_model}, such as choice, parallel composition, sequential composition and \textsc{Kleene} star. However, as we will see, some of the data constructors will be more general and thus more expressive and flexible than the operators presented in \chpref{chp:algebraic_model}.

At this point, we want to remark that that it is very well possible to wrap a complicated function into a \texttt{Const} or \texttt{Simple} process. However, in terms of the \textsc{Hive} process algebra, we regard them as atomic processes since they do not involve any process combinators.

\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Local} type constructor.,numbers=left,frame=bt]
Local :: (Serializable b) 
      => Process a b
      -> Process a b
\end{lstlisting}
\texttt{Local} simply wraps a Hive process without altering its semantics. The structure of \texttt{Local} is that of a decorator \cite{Gamma:1995:DPE:186897}, its purpose is to give an indication to the process interpreter\footnote{We will have a closer look at the interpreter in \chpref{chp:implementation}. For now, the reader is invited to accept the existence of an interpreter that takes care of the distribution and execution of Hive process structures.} that the wrapped process should be executed locally. Typically, the reason for this arises from the expectation that serialising a closure, sending it to a remote node, executing it there and obtaining the result is more expensive\footnote{Expensive in terms of the neccessary amount of time to run the process.} than executing the respective process locally. Since there is no general approach to estimate the neccessary amount of time to run a process, we decide to equip the user with the \texttt{Local} type constructor and burden him with the obligation to make appropriate use of it.

\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Choice} type constructor.,numbers=left,frame=bt]
Choice :: (Serializable b)
       => c
       -> (a -> c -> d)
       -> (d -> Bool)
       -> Process a b
       -> Process a b
       -> Process a b
\end{lstlisting}
The \texttt{Choice} data constructor is used to build a process that makes a choice between two processes an represents the choice operator $\vee$. Unlike in the algebraic model from \chpref{chp:algebraic_model}, this choice is not made non-deterministically by an oracle, but based on a predicate. This changes the semantics of \texttt{Choice} in contrast to $\vee$: we do not create a process that non-deterministically behaves like two processes at the same time. The introduced predicate makes sure that the process' behaviour is always determined and the environment is always in exactly one state. \texttt{Choice} takes a value of type \texttt{c}, a function of type \texttt{a $\to$ c $\to$ d}, a predicate \texttt{d $\to$ Bool} and two processes of type \texttt{Process a b}. The interplay of the additional parameters will become clear in \chpref{chp:implementation}. For now, it will suffice to say that they enable process choice based on a static or dynamic value.

\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Seqeuence} type constructor.,numbers=left,frame=bt]
Sequence :: (Serializable b, Serializable c)
         => Process a c
         -> Process c b
         -> Process a b
\end{lstlisting}
The \texttt{Sequence} data constructor takes two processes and composes them sequentially, it represents the sequence operator $\circ$. The result types of both processes need to have an instance of \texttt{Serializable} and the input type of the second process must correspond to the output type of the first process.

\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Parallel} type constructor.,numbers=left,frame=bt]
Parallel :: (Serializable b, Serializable c, Serializable d)
         => Process a c
         -> Process a d
         -> Process (c, d) bs
         -> Process a b
\end{lstlisting}
With the \texttt{Parallel} data constructor we can create a \texttt{Process} that runs two processes in parallel. As the signature of \texttt{Parallel} shows, our model for parallel composition of processes is more general than the one introduced in \chpref{chp:algebraic_model}. Where in the algebraic model from \chpref{chp:algebraic_model}, we can only combine two processes with the parallel combinator $|$ iff they have the same type signature and their result type forms a semigroup with a commutative binary operation, we can use arbitrary processes for parallel composition in the \textsc{Hive} process algebra, however, they need to accept the same input type. \texttt{Parallel} takes two processes, with types \texttt{Process a c} and \texttt{Process a d}, that are to be composed in a parallel way. In addition to that, it takes a third process of type \texttt{Process (c, d) b}, and uses it to combine the results of the other two processes after they have finished their execution. The user may be well advised to consider wrapping the combinator process in a \texttt{Local} process since this may potentially be a cheap operation. Note that using an explicit process to combine the results is exactly what enables us to compose processes of different types in parallel. At the same time we eliminate the required structure of a semigroup on their result type.

\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Multilel} type constructor.,numbers=left,frame=bt]
Multilel :: (Serializable b, Serializable c)
         => [Process a c]
         -> b
         -> Process (b, [c]) b
         -> Process a b
\end{lstlisting}
\texttt{Multilel} represents the generalisation of parallel composition of two processes to parallel composition of an arbitrary number of processes. It takes a list of processes of type \texttt{Process a c}, a value of type \texttt{b} and a process of type \texttt{Process (b, [c]) b}. The list of processes contains the processes that should be composed in parallel, the additional process together with the value of type \texttt{b} is used to fold the results of the processes together. Again, the user may be well advised to wrap the combinator process into a \texttt{Local} process in order to save runtime that would be spent for serialisation and sending data over a network. 

\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Loop} type constructor.,numbers=left,frame=bt]
Loop :: (Serializable b)
     => b
     -> c
     -> (c -> Bool)
     -> (b -> a)
     -> (a -> c -> c)
     -> Process a b
     -> Process a b
\end{lstlisting}
With the \texttt{Loop} data constructor, we can wrap a process for repeated execution, resembling the \textsc{Kleene} star $*$. However, we provide a much more sophisticated and general version than the \texttt{Kleene} star does or than can be found in imperative programming languages. First, \texttt{Loop} takes a value of type \texttt{b}, which serves as a default output value in case the loop is executed exatly zero times. In imperative programming, this is the implicitly unchanged global state of the program. Then it takes a value of type \texttt{c}, a predicate of type \texttt{c $\to$ Bool} and a function of type \texttt{a $\to$ c $\to$ c}. The combination of these three controls the termination of the \texttt{Loop}'s execution. The function of type \texttt{b $\to$ a} is used to convert the result of the process of type \texttt{Process a b} back to a value of type \texttt{a}, so it can be fed into the next execution of the \texttt{Loop}. Note that our \texttt{Loop} does not necessarily behave like the identity process $Id$ if it runs zero times. Responsible for this is the explicit default value of type \texttt{b} that will be returned in this case. However, if this value represents the \texttt{Loop}'s environment before execution, the \texttt{Loop} process would behave like the \texttt{Id} process in case it runs zero times. 

\section{Implementation}
\label{chp:implementation}
We kindly want to remind the reader about the goal we set in \chpref{chp:goal}, i.e. the automatic distribution of processes in a distributed system while hiding the fact that a distributed system is involved and thereby making distributed programming transparent and easy. As mentioned earlier, we are going to employ the interpreter pattern \cite{Gamma:1995:DPE:186897} for this purpose. But to do so, first we need some kind of infrastructure and nodes where we can delegate the work to.

\subsection{Architecture}
The architecture of our system's infrastructure is intentionally kept simple. It involves a designated \textbf{master} node, a collection of \textbf{worker} nodes and a RESTful web interface for the exchange of JSON documents that is realised using Yesod\footnote{Yesod is a Haskell web frameword. More information about Yesod can be found at \url{http://www.yesodweb.com/} and \url{https://github.com/yesodweb/yesod}}.

When a client wants to submit a request, it has to make use of the web interface and supply a JSON document in an appropriate format. In a productive environment, there would be a parser that reads a submitted JSON document, generates a corresponding process structure from its content and passes it on to the process interpreter if the document is valid. For simplicity, we will assume for now that a request equals a valid process description that can be interpreted.

The master node is responsible for handling client requests, logging information related to the received requests and managing connected nodes. When a request is received, the master will spawn a new \texttt{CH.Process} that runs the \textsc{Hive} process interpreter and pass the request to the new process where it will be interpreted. At the same time, the master assigns a ticket id to the request, logs the request together with its id to a database\footnote{For this purpose, we make use the \texttt{acid-state} library. \texttt{acid-state} allows to save Haskell values into a file-based database if their type has an instance of \texttt{SafeCopy}. Further information on \texttt{acid-state} can be found at \url{http://acid-state.seize.it/} and \url{https://github.com/acid-state/acid-state}} and reports the id to the client. When the interpreter finishes interpreting the process structure from the request, it reports the result to the master. The master then updates the database by logging the result to the database and linking it to the according id. At any time, a client can try to retrieve the result to its request by supplying the request's ticket id to the master. The master then tries to read the result from the database, but will only reply to the client's satisfaction after the interpretation of its request has been finished and logged to the database.

Worker nodes are kept very simple. When started and supplied with the master's address, they report their availability to the master and wait for further instruction.

The \textsc{Hive} process interpreter, which we will discuss in full detail in \chpref{chp:interpreter}, is run in a new process by the master for every client request that is received. The interpreter inspects the structure of the received process and distributes the incorporated sub-processes to worker nodes or executes them locally if they have been marked for local execution using the \texttt{Local} wrapper. For every sub-process, the interpreter asks the master for a worker node where it can run the sub-process. For such a request, the master fetches a worker node from a FIFO queue and returns it to the interpreter. After a sub-process has finished execution on a worker node, the interpreter returns the respective worker node to the master for future allocation to other interpreters. It would very well be possible to employ a more sophisticated scheduling algorithm, but the combination of a FIFO queue and a work stealing like \cite{} behaviour of the process interpreter is fairly efficient and easy to implement. Our primary goal is not to achieve the best possible load balancing or employ the most efficient technique for queueing worker nodes, however this leaves room for future improvement.

\subsection{The \textsc{Hive} process interpreter}
\label{chp:interpreter}
In this chapter we will discuss the \textsc{Hive} process interpreter and how its interpretation of \textsc{Hive} processes works in detail by taking a look at the concrete implementation.

When the master node receives a client request, it starts a process interpreter in a fresh process on the node it is running on itself and passes the process structure from the request to the interpreter. The interpreter then inspects the process structure and distributes the incorporated sub-processes to connected worker nodes accordingly. To do so, the interpreter asks the master node for an available worker node for every sub-process that has to be executed. We represent the master node by a \texttt{ProcessId} that is wrapped into a value of type \texttt{Master}.
\begin{lstlisting}[language=Haskell,caption=Data type for the address of a master node.,numbers=left,frame=bt]
newtype Master = Master ProcessId
\end{lstlisting}

In our implementation, the name of the interpreter function is \texttt{runProcess} and its type signature is as shown in \lstref{lst:interpreter_signature}. It takes a value of type \texttt{Master}, a \textsc{Hive} process of type \texttt{Process a b}, an argument for the \textsc{Hive} process of type \texttt{a} and behaves like a \textsf{Cloud Haskell} process, i.e. when executed in the \textsf{Cloud Haskell} \texttt{CH.Process} Monad, it produces a value of type \texttt{b}.
\begin{lstlisting}[language=Haskell,caption=Signature of the process interpreter.,label=lst:interpreter_signature,numbers=left,frame=bt]
runProcess :: Master -> Process a b -> a -> CH.Process b
\end{lstlisting}

For the interpretation of a \texttt{Const} process, we take the \texttt{sDict} value of type \texttt{SerializableDict b} and the \texttt{closure} of type \texttt{CH.Closure (CH.Process b)} from the \texttt{Const} process and create a new \texttt{Simple} process out of that. For that we can simply keep the \texttt{sDict} as it is but need to turn the \texttt{closure} into a closure generator, i.e. a function that takes a value of type \texttt{a} and generates a closure from it. Since we're dealing with a \texttt{Const} process which simply discards its input value and always behaves the same, we have to respect this when creating the closure generator for the new \texttt{Simple} process. This can be done by using Haskell's standard function \texttt{const} which takes two values, discards the second one and always returns the first. Then, all we have to do is pass the new \texttt{Simple} process into the process interpreter.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter of \texttt{Const} processes.,label=lst:runprocess_const,numbers=left,frame=bt]
runProcess master (Const sDict closure) x =
  runProcess master (Simple sDict (const closure)) x
\end{lstlisting}

When encountering a \texttt{Simple} process in a process structure, the process interpreter asks the master node for an available worker node where it can run this basic process, as shown in line 2 of \lstref{lst:runprocess_simple}. This operation will block until the master node is able to satisfy the request for an available worker node and supplies it to the interpreter. The interpreter then uses the closure generator \texttt{closureGen}, which has type \texttt{a $\to$ CH.Closure (CH.Process b)}, to generate a closure that is then serialised and sent to the available worker node for execution, as shown in line 3 of \lstref{lst:runprocess_simple}. This operation blocks the interpreter until execution of the remotely spawned process on the worker node has terminated and a result value has been obtained. After that, the worker node is returned to the master node and the result value that has been calculated on it is returned by the process interpreter.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Simple} processes.,label=lst:runprocess_simple,numbers=left,frame=bt]
runProcess master (Simple sDict closureGen) x = do
  node <- getNode master =<< getSelfPid
  res  <- call sDict node (closureGen x)
  returnNode master node
  return res
\end{lstlisting}

When the process interpreter encounters a process that is wrapped into a \texttt{Local} wrapper, it is supposed to execute it locally on the same node it is running on itself instead of distributing it or possibly involved sub-processes to remote notes. To accomplish this behaviour, we apply a little trick: we create a fake master, as shown in line 2 of \lstref{lst:runprocess_local}, that will always answer with the local node when asked for an available worker node and we're discarding the real master node that is passed as first argument to \texttt{runProcess}. We use this fake master to pass it to a recursive call of \texttt{runProcess} that interprets the process that is wrapped by the \texttt{Local} wrapper at hand. Thereby, we're making sure that for the interpretation of the whole tree of sub-process this fake master will be used and they will be interpreted locally. After the interpretation of the process structure has been finished, we terminate the fake master we had created before.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Local} processes.,label=lst:runprocess_local,numbers=left,frame=bt]
runProcess _ (Local p) x = do
  fakeMaster <- getFakeMaster =<< getSelfPid
  res <- runProcess fakeMaster p x
  terminateMaster fakeMaster
  return res
\end{lstlisting}

Unlike in our algebraic model, specifically \defref{def:static_choice} and \defref{def:sem_choice}, a \texttt{Choice} process in the \textsc{Hive} process algebra does not introduce non-determinism. Earlier we used an optimistic oracle to make the choice between two prcesses, but didn't have any control over the oracle's behaviour, thus introducing non-determinism. In order to get rid of the non-determinism we replace the non-deterministic oracle with a deterministic predicate \texttt{p} of type \texttt{d $\to$ Bool} that makes the choice between processes, as shown in \lstref{lst:runprocess_choice}. The predicate's choice is based on a value \texttt{c} and the arguments for the process \texttt{x} of type \texttt{a}. These two values are combined using a function \texttt{acd :: a $\to$ c $\to$ d} and the result is presented to the predicate \texttt{p}. If \texttt{p (acd x c)} holds, the first process \texttt{p1} is selected for execution and the second process \texttt{p2} otherwise. Note that we have eliminated one of the two places where there was non-determinism introduced in the algebraic model.  % ToDo: mention intention of deterministic processes
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Choice} processes.,label=lst:runprocess_choice,numbers=left,frame=bt]
runProcess master (Choice c acd p p1 p2) x =
  runProcess master (if p (acd x c) then p1 else p2) x
\end{lstlisting}

For the interpretation of a \texttt{Sequence} process, we have to run the first process \texttt{p1} on the argument \texttt{x} first by making use of the interpreter function \texttt{runProcess}. Then, using the result from \texttt{p1}, we run the second process \texttt{p2}. The implementation of this is straightforward, using the bind operator \texttt{>}\texttt{>=} from the \texttt{CH.Process} monad, as shown in \lstref{lst:runprocess_sequence}.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Sequence} processes.,label=lst:runprocess_sequence,numbers=left,frame=bt]
runProcess master (Sequence p1 p2) x =
  runProcess master p1 x >>= runProcess master p2
\end{lstlisting}

In the implementation of the interpreter function \texttt{runProcess} for the previous \textsc{Hive} process types, i.e. \texttt{Const}, \texttt{Simple}, \texttt{Local}, \texttt{Choice} and \texttt{Sequence}, there has been no parallelism introduced. This was exactly the desired behaviour and we could simple made use of the blocking behaviour of the underlying \textsf{Cloud Haskell} framework. However, the purpose of the \textsc{Hive} process algebra is to make distributed programming easy and intuitive. Hence, we need a mechanism to introduce parallelism. This is done with the \texttt{Parallel} and \texttt{Multilel} combinators for processes. Whereas \texttt{Parallel} allows for parallel composition of two \textsc{Hive} processes with arbitrary types, \texttt{Multilel} allows for parallel composition of arbitrary many \textsc{Hive} processes of a fixed type.

So far, we could simply perform the interpretation of processes in the interpreter thread\footnote{We are going to use the word \textbf{thread} intentionally here to avoid confusion about which \textbf{process} would be meant otherwise. Strictly technically speaking, this use of words is even correct. In \textsf{Cloud Haskell}, spawning a local \textbf{process} is done by forking a new \textbf{thread}.} because we only had one program flow. This changes in the case of the \texttt{Parallel} and \texttt{Multilel} processes where we explicitly have two or more program flows. That means we have to fork at least one more local thread for parallel interpretation of the processes. Remember that \textsc{Hive} processes do not have a \texttt{Serializable} instance and therefore process interpretation needs to happen locally and can \textbf{not} be done remotely. We introduce an auxiliary process \texttt{runProcessHelper} for this. It takes a \texttt{Master}, a \textsc{Hive} process of type \texttt{Process a b}, an argument for the process of type \texttt{a} and an \texttt{MVar b} that will be used to communicate the result. \texttt{runProcessHelper} runs in the \textsf{Cloud Haskell} \texttt{CH.Process} monad and its behaviour is fairly simple: it runs the process interpreter \texttt{runProcess} for the submitted process by passing the argument to it and saving the obtained result in the \texttt{mvar}, then it terminates. The implementation of \texttt{runProcessHelper} can be found in \lstref{lst:runprocesshelper}.
\begin{lstlisting}[language=Haskell,caption=Auxiliary process for the interpretation of \texttt{Parallel} and \texttt{Multilel} processes.,label=lst:runprocesshelper,numbers=left,frame=bt]
runProcessHelper :: Master
                 -> Process a b
                 -> a
                 -> MVar b
                 -> CH.Process ()
runProcessHelper master p x mvar = do
  r <- runProcess master p x
  liftIO $ putMVar mvar r
\end{lstlisting}

\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Parallel} processes.,numbers=left,frame=bt]
runProcess master (Parallel p1 p2 combinator) x = do
  mvar <- liftIO newEmptyMVar
  spawnLocal $ runProcessHelper master p1 x mvar
  r2 <- runProcess master p2 x
  r1 <- liftIO $ takeMVar mvar
  runProcess master combinator (r1, r2)
\end{lstlisting}

\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Multilel} processes.,numbers=left,frame=bt]
runProcess master (Multilel ps ib fold) x = do
  mvars <- forM ps $ \_ -> liftIO newEmptyMVar
  mapM_ (\(proc,mvar) -> spawnLocal $ runProcessHelper master proc x mvar) (ps `zip` mvars)
  ress  <- forM mvars $ \m -> (liftIO . takeMVar $ m)
  runProcess master fold (ib, ress)
\end{lstlisting}

\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Loop} processes.,numbers=left,frame=bt]
runProcess master (Loop ib ic pr ba acc p) x =
  if pr (acc x ic) then do
    x' <- runProcess master p x
    runProcess master (Loop x' (acc x ic) pr ba acc p) (ba x')
  else
    return ib
\end{lstlisting}

\section{Examples}
\label{chp:example}

\subsection{Hello world for interpreters}

\subsection{An ant system for the TSP}