\chapter{Implementation of the model\index{Implementation}}
In this chapter we describe two implementations\index{Implementation} of the algebraic model from \chpref{chp:algebraic_model} in \textsf{Haskell}\index{Haskell}. We closely resemble the structure and semantics of the algebraic model in our data model in \textsf{Haskell}, so it is straightforward to transform process structures formulated in \texttt{Haskell} into a representation in our model and reason about them. As a useful consequence, the laws stated in \chpref{chp:laws} that are valid for our model transform to processes in our implementation and remain valid.

For one of the implementations, we use \textsf{Haskell}'s \texttt{IO} monad and fork local threads for local parallelisation on one computer. For the other one, we use \textsf{Cloud Haskell} to distribute processes into a distributed system and run them in \textsf{Cloud Haskell}'s \texttt{Process} monad.

Ideally, we want the two implementations to be usable interchangeably by simply importing the desired implementation and without changing any of the productive code, however we cannot achieve this at the moment. When using the implementation based on the \textsf{Cloud Haskell} \texttt{Process} monad, we need to require additional properties of the data types that are transmitted over the network as inputs and outputs of processes. Furthermore, in order to run code remotely, the code has to be known on the remote node since \textsf{ghc}\footnote{The Glasgow Haskell Compiler, c.f. \url{https://www.haskell.org/ghc/}.} doesn't support serialising and transmitting functions over the network \cite{Epstein:2011:THC:2034675.2034690}. To solve this problem, a trick is used that requires us to change the model of basic processes slightly. However, we are optimistic that with future versions of \textsf{ghc} it will be possible to achieve full interchangeability of both (and possibly more) implementations.

\nomenclature{ghc}{The Glasgow Haskell Compiler}


\clearpage

\section{Implementation in the IO monad}
\label{chp:local}
In the implementation for local parallelisation on only one computer, we make use of \textsf{Haskell}'s \texttt{IO} monad and the \texttt{Control.Concurrent} package. Basic processes are represented as computations in the \texttt{IO} monad. Parallelisation is achieved by running processes that have been modelled for parallel execution in lightweight threads using \texttt{forkIO} and synchronisation is done using \texttt{MVar}s.

We do not discuss the basic principles of the \texttt{IO} monad here, an introduction can be found in standard \textsf{Haskell} literature such as \cite{Hutton} or \cite{Bird}. However, the fact that we use the \texttt{IO} monad has immediate consequences: we \textbf{can not} force processes to be free of side-effects\index{Side-effect} and they can perform actions like e.g. reading from or writing to the file system.

\subsection{Data model}
\label{chp:local_model}
Our data model resembles the structure of the algebraic model\index{Algebraic!model} given in \chpref{chp:algebraic_model}. We aim to prevent the creation of erroneous processes and check for their validity at compile time. To achieve this, we employ a generalised algebraic data type\index{Generalised algebraic data type} and leverage the power of \textsf{Haskell}'s type system. 

As mention before, basic processes are computations in the \texttt{IO} monad. They receive an input of type \texttt{a} and output a value of type \texttt{b}.
\begin{lstlisting}[language=Haskell,caption=Representation of basic processes as computations in the \texttt{IO} monad.,label=fig:local_computation,numbers=left,frame=bt]
type BasicProcess a b = a -> IO b
\end{lstlisting}

Predicates are represented as processes that receive an input and output a value of type \texttt{Bool}, as stated in \defref{def:predicate}.
\begin{lstlisting}[language=Haskell,caption=Representation of predicates as processes.,label=fig:local_computation,numbers=left,frame=bt,firstnumber=2]
type Predicate a = Process a Bool
\end{lstlisting}

The data type for processes involves two type parameters\index{Type!parameter}, \texttt{a} and \texttt{b}. They reflect the process' input and output types where \texttt{a} is the type of the input and \texttt{b} is the type of the output.
\begin{lstlisting}[language=Haskell,caption=Data type for the representation of processes.,label=fig:local_datatypes,numbers=left,frame=bt,firstnumber=2]
data Process a b where
\end{lstlisting}

The identity process and the error process are modelled using the \texttt{Id}\index{Data constructor!Id} and the \texttt{Err}\index{Data constructor!Err} data constructors. Their type signatures are as stated in \defref{def:static_id_err}.
\begin{lstlisting}[language=Haskell,caption=Data constructors for the id and error process.,label=fig:local_datatypes,numbers=left,frame=bt,firstnumber=2]
Id  :: Process a a
Err :: Process a a
\end{lstlisting}

Basic processes are turned into processes by wrapping them using the \texttt{Basic}\index{Data constructor!Basic} data constructor. The developer is cautioned about using basic processes that incorporate side-effects as they potentially render the laws introduced in \chpref{chp:laws} invalid. \texttt{Basic}\index{Data constructor!Basic} takes a basic process\index{Process!Basic} of type \texttt{BasicProcess a b}, i.e. a computation in the \texttt{IO} monad that takes an input of type \texttt{a} and outputs a value of type \texttt{b}.
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Simple} data constructor.,numbers=left,frame=bt,firstnumber=5]
Basic :: BasicProcess a b
      -> Process a b
\end{lstlisting}

The \texttt{Choice}\index{Data constructor!Choice} data constructor is used to build a process that makes a choice between two processes. Its type signature satisfies \defref{def:static_choice}: \texttt{Choice} takes a predicate of type \texttt{Predicate a}, two processes of type \texttt{Process a b} and results in a process of type \texttt{Process a b}.
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Choice} data constructor.,numbers=left,frame=bt,firstnumber=7]
Choice :: Predicate a
       -> Process a b
       -> Process a b
       -> Process a b
\end{lstlisting}

The \texttt{Parallel}\index{Data constructor!Parallel} data constructor is used to model parallel composition of processes, its type signature reflects \defref{def:static_parallel}. It takes two processes for parallel composition, with type signatures \texttt{Process a c} and \texttt{Process a d}, as well as a third process of type \texttt{Process (c, d) b} that is used to combine the results of the other two processes.
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Parallel} data constructor.,numbers=left,frame=bt,firstnumber=16]
Parallel :: Process (c, d) b
         -> Process a c
         -> Process a d
         -> Process a b
\end{lstlisting}

The \texttt{Sequence}\index{Data constructor!Sequence} data constructor takes two processes for sequential composition. Just as for conventional function composition, the output type of the first process and the input type of the second process must coincide, as stated in \defref{def:static_sequence}.
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Seqeuence} data constructor.,numbers=left,frame=bt,firstnumber=13]
Sequence :: Process a c
         -> Process c b
         -> Process a b
\end{lstlisting}

With the \texttt{Repetition}\index{Data constructor!Repetition} data constructor, a process can be wrapped for repeated execution. \texttt{Repetition} takes a predicate of type \texttt{Predicate a} and a process of type \texttt{Process a a}, as defined in \defref{def:static_repetition}.
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Repetition} data constructor.,numbers=left,frame=bt,firstnumber=34]
Repetition :: Predicate a
           -> Process a a
           -> Process a a
\end{lstlisting}

ToDo: Explain Multilel as special case of hierarchy of Choice processes with specific combinator.\\
\texttt{Multilel}\index{Data constructor!Multilel} represents the generalisation of parallel composition of two processes to parallel composition of an arbitrary number of processes. It takes a list of processes of type \texttt{Process a c}, a value of type \texttt{b} and a process of type \texttt{Process (b, [c]) b}. The list of processes contains the processes that should be composed in parallel, the additional process together with the value of type \texttt{b} is used to fold the results of the processes together.
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Multilel} data constructor.,numbers=left,frame=bt,firstnumber=20]
Multilel :: [Process a c]
         -> Process [c] b
         -> Process a b
\end{lstlisting}

\subsection{Process interpreter}
In this chapter we discuss how the \texttt{Process} interpreter works in detail by taking a look at its implementation.

Processes can be applied to an input using the \texttt{runProcess} function: it takes a process of type \texttt{Process a b} and an input of type \texttt{a} for that process. It runs in the \texttt{IO} monad and produces a result of type \texttt{b}. \texttt{runProcess} is the implementation of the function $sem$ introduced in \chpref{chp:semantics} that defines the semantics of a process.
\begin{lstlisting}[language=Haskell,caption=Signature of the process interpreter.,label=lst:local_runprocess_signature,numbers=left,frame=bt]
runProcess :: Process a b -> a -> IO b
\end{lstlisting}

The semantics of an \texttt{Id}\index{Process interpreter!Id} process is as given in \defref{def:sem_id}: it simply outputs its input.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter of \texttt{Id} processes.,label=lst:local_runprocess_const,numbers=left,frame=bt,firstnumber=2]
runProcess Id x =
  return x
\end{lstlisting}

The semantics of an \texttt{Err}\index{Process interpreter!Err} process is as stated in \defref{def:sem_err}. The error process outputs the undefined value regardless of its input.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter of \texttt{Id} processes.,label=lst:local_runprocess_const,numbers=left,frame=bt,firstnumber=2]
runProcess Err _ =
  return undefined
\end{lstlisting}

A \texttt{Basic}\index{Process interpreter!Basic} process is simply a wrapper for a computation \texttt{f} in the \texttt{IO} monad, which is the equivalent of an intrinsic function of a basic process as described in \chpref{chp:semantics}. The interpretation of a \texttt{Basic} process is done by applying the process' intrinsic function \texttt{f} to the process' input \texttt{x}, as given in \defref{def:sem_atomic}.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Basic} processes.,label=lst:local_runprocess_simple,numbers=left,frame=bt,firstnumber=4]
runProcess (Basic f) x =
  f x
\end{lstlisting}

For the interpretation of \texttt{Choice} processes, the output of \texttt{predicate} when supplied with input \texttt{x} has to be obtained first by making use of the interpreter function \texttt{runProcess}. Then, based on the predicate's output, the choice between executing either \texttt{p1} or \texttt{p2} is made as defined in \defref{def:sem_choice}.
\index{Process interpreter!Choice}
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Choice} processes.,label=lst:local_runprocess_choice,numbers=left,frame=bt,firstnumber=6]
runProcess (Choice predicate p1 p2) x = do
  b <- runProcess predicate x
  runProcess (if b then p1 else p2) x
\end{lstlisting}

A \texttt{Parallel}\index{Process interpreter!Parallel} process involves two process that have to be interpreted at the same time, namely \texttt{p1} and \texttt{p2}, as well as a combinator process \texttt{combinator} to combine the outputs of \texttt{p1} and \texttt{p2}. In order to interpret two processes at the same time, a second thread needs to be forked and the interpretation of one of the processes has to be delegated there. To do so, the auxiliary function \texttt{runProcessHelper} is implemented: \texttt{runProcessHelper} takes a process \texttt{p} that should be interpreted, an input \texttt{x} for \texttt{p} and an mvar. \texttt{runProcessHelper} determined the output of \texttt{p} applied to \texttt{x} by making use of \texttt{runProcess} and saves the output to the mvar.
\begin{lstlisting}[language=Haskell,caption=Auxiliary function for the interpretation of \texttt{Parallel} processes.,label=lst:local_runprocesshelper,numbers=left,frame=bt,firstnumber=10]
runProcessHelper :: Process a b -> a -> MVar b -> IO ()
runProcessHelper p x mvar = putMVar mvar =<< runProcess p x
\end{lstlisting}

To interpret a \texttt{Parallel} process, an empty mvar is created and, together with \texttt{p1} and input \texttt{x}, passed on to \texttt{runProcessHelper} that is forked in a new thread using \texttt{forkIO}. While the output of \texttt{p1} is determined in the new thread, the interpreter determines the output of \texttt{p2} in its local thread. The previously created mvar is used to comfortably wait for the output of \texttt{p1} that \texttt{runProcessHelper} saves to it once it has been obtained. Finally, \texttt{combinator} is used to combine the outputs of \texttt{p1} and \texttt{p2} into a single value, as given in \defref{def:sem_parallel}.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Parallel} processes.,numbers=left,frame=bt,label=lst:local_runprocess_parallel,firstnumber=12]
runProcess (Parallel p1 p2 combinator) x = do
  mvar <- newEmptyMVar
  _    <- forkIO $ runProcessHelper p1 x mvar
  r2   <- runProcess p2 x
  r1   <- takeMVar mvar
  runProcess combinator (r1, r2)
\end{lstlisting}

For the interpretation of a \texttt{Sequence}\index{Process interpreter!Sequence} process, \texttt{runProcess} is employed to determine the output of \texttt{p1} for input \texttt{x}. Then, the output of \texttt{p1} if used as input for the interpretation of \texttt{p2} with \texttt{runProcess}, as defined in \defref{def:sem_sequence}. The implementation of this is straightforward, using the bind operator \texttt{>}\texttt{>=}.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Sequence} processes.,label=lst:local_runprocess_sequence,numbers=left,frame=bt,firstnumber=8]
runProcess (Sequence p1 p2) x =
  runProcess p1 x >>= runProcess p2
\end{lstlisting}

\clearpage
Finally, there is the \texttt{Repetition}\index{Process interpreter!Repetition} process that is used to express repeated execution of a process. As stated in \defref{def:sem_repetition}, repetition is expressed using a combination of choice composition and sequence composition.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Repetition} processes.,numbers=left,frame=bt,firstnumber=23]
runProcess rep@(Repetition pr p) x =
  runProcess (Choice pr (Sequence p rep) Id) x
\end{lstlisting}


ToDo: rephrase...\\
The interpretation of a \texttt{Multilel}\index{Process interpreter!Multilel} process follows the same principles as the interpretation of a \texttt{Parallel} process. The difference only lies in that we have to deal with a list of processes that should be run in parallel instead of exactly two. This has some consequences: we require all of these processes to have the same type, otherwise we would end up with $n-1$ combinator processes for the combination of the results from $n$ processes. By requiring the same type for every process and thus, the same result type, we can reduce the requirement for combinator processes to only one process, namely \texttt{fold}, and one value, namely \texttt{ib}, as can be seen in \lstref{lst:local_runprocess_multilel}. As the name \texttt{fold} suggests, this process behaves like a fold: it takes a pair that contains the initial fold value \texttt{ib} and a sequence of values, namely \texttt{ress}, that should be folded together into one result value. For parallel execution of all the processes in \texttt{ps}, we create a new empty \texttt{MVar} for each of them and start a \texttt{runProcessHelper} in a new thread for each of them, supplying one of the \texttt{mvars} per thread. Then we wait for all of the processes to finish execution by reading their results from the \texttt{mvars}. Should there be even a single process that hasn't finished execution, the according \texttt{MVar} blocks the reading action until a value has been saved into it. Finally, the \texttt{fold} process is run, folding the results together and returning a single result.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Multilel} processes.,label=lst:local_runprocess_multilel,numbers=left,frame=bt,firstnumber=18]
runProcess (Multilel ps ib fold) x = do
  mvars <- forM ps $ const newEmptyMVar
  mapM_ (\(p, m) -> forkIO $ runProcessHelper p x m) (ps `zip` mvars)
  ress  <- forM mvars takeMVar
  runProcess fold (ib, ress)
\end{lstlisting}

\clearpage


\section{Implementation in the Cloud Haskell Process monad}
\label{chp:distributed}
In the implementation for distributed parallelisation of process interpretation, we use \textsf{Cloud Haskell}'s \texttt{Process} monad. The involvement of network communication requires that we provide instances of specific type classes for the types that are to be sent over network. Furthermore, we need to adapt our representation of basic processes and some process types to the specifics of \textsf{Cloud Haskell}. 

\subsection{\index{Cloud Haskell}Cloud Haskell}
\label{chp:cloud_haskell}
\textsf{Cloud Haskell} \cite{Epstein:2011:THC:2034675.2034690} is a domain specific language for distributed programming in Haskell. It is highly inspired by Erlang\index{Erlang} and its message passing\index{Message passing} mechanism for communication between processes. Like it is typical for \textsf{Haskell}, there is no implicitly shared memory involved.

A \textsf{Cloud Haskell} process is essentially a function\index{Function} that is evaluated in the \texttt{Process} monad\index{Monad} and can be spawned on a local\index{Node!local} or remote\index{Node!remote} node. Processes can send messages to other processes if they have knowledge about their process identifier, which serves as an address. The \texttt{Process} monad builds on top of the \texttt{IO} monad and thus, as mentioned in \chpref{chp:local}, we can not force processes to be free of side-effects.

While Erlang\index{Erlang} uses atoms as tags for messages, \textsf{Cloud Haskell} uses data types that need to have an instance of \texttt{Serializable}. \texttt{Serializable} itself is only a combination of both \texttt{Binary} and \texttt{Typeable}. \texttt{Binary} is neccessary to serialise a message into a \texttt{ByteString}, \texttt{Typeable} is used to identify the type of a value. This way, serialisation\index{Serialisation} is made explicit, in contrast to Erlang where it is implicit \cite{Epstein:2011:THC:2034675.2034690}.

In \textsf{Haskell}, functions can only be executed, composed and passed as arguments, they cannot be serialised. However, this is be necessary in order to send a function to a remote\index{Node!remote} node and execute it there. \textsf{Cloud Haskell} avoids this problem by using a table of static code pointers, i.e. fully qualified top level names of functions that are known at compile time, to refer to functions by a name. For remote execution, a function's name is put into a \texttt{Closure}\index{Closure}, together with its serialised environment\index{Environment}, i.e. its argument, and sent to a remote node where it is deserialised and executed. A \texttt{Closure} is nothing more than just mentioned: a function (name) together with its argument \cite{Epstein:2011:THC:2034675.2034690} and can be created using \textsf{Cloud Haskell}'s function \texttt{mkClosure} and \textsf{Template Haskell}. \texttt{mkClosure} takes the top level name of a function and returns a closure generator. The function supplied to \texttt{mkClosure} needs to be of 1-ary type \texttt{a $\to$ Process b} and return a \textsf{Cloud Haskell} process. For a function \texttt{f} of type \texttt{a $\to$ Process b}, the type of \texttt{\$(mkClosure 'f)} resolves to \texttt{a $\to$ Closure (Process b)}, i.e. it is a closure generator that generates a closure when supplied with a value of type \texttt{a}.

After a \texttt{Closure} has been executed, the result is serialised and sent back to the caller. However, in some cases, the type system cannot infer the serialisability of the result type and therefore additional information needs to be provided\footnote{Specifically, the problem is that the type constructors can be constrained with required type class instances for parameters. On deconstruction, this information is not available and therefore needs to be added explicitly again.}. For a type \texttt{a}, serialisation information can be provided with a value of type \texttt{Static (SerializableDict a)}. Essentially this is only an explicit type tag that enables the selection of the correct serialisation function for type \texttt{a}.

\subsection{Data model}
\label{chp:distributed_model}
The data model for the representation of processes in the \textsf{Cloud Haskell} \texttt{Process} monad is essentially the same as what we saw in \chpref{chp:local_model}. Predicates are still processes that output a boolean value and basic processes are computations in the \textsf{Cloud Haskell} \texttt{Process} monad. In the following \texttt{CH} is the name for the qualified import of \texttt{Control.Distributed.Process}.
\begin{lstlisting}[language=Haskell,caption=Representation of basic processes as computations in the \texttt{Process} monad.,numbers=left,frame=bt]
import qualified Control.Distributed.Process as CH

type BasicProcess b = CH.Process b
\end{lstlisting}

The signature of our representation of processes remains unchanged, only the data constructors need to be changed slightly.
\begin{lstlisting}[language=Haskell,caption=Data type for the representation of processes.,label=fig:distributed_datatypes,numbers=left,frame=bt,firstnumber=4]
data Process a b where
\end{lstlisting}

The wrappers for basic processes, i.e. \texttt{Const} and \texttt{Simple} reflect the necessary adaptations that come with \textsf{Cloud Haskell}: they constrain the output type of the wrapped basic process to be an instance of the type class \texttt{Serializable} and require a static \texttt{SerializableDict} for the same type. As mentioned in \chpref{chp:cloud_haskell}, this is necessary so remotely\footnote{This is true for locally spawned processes as well, as we can conceptually not distinguish between them: \textsf{Cloud Haskell} transparently takes care of that we can use both local and remote processes in the same way. Technically, however, we could inspect the process identifier and tell if the respective process is running on the same or a different node.} spawned processes know how to serialise their output and send it back to the caller. Instead of a value of type \texttt{BasicProcess} as before, \texttt{Const} takes a closure that contains a basic process. Similarly, \texttt{Simple} takes a closure generator, i.e. a function that, supplied with a value of the appropriate type, generates a closure. %ToDo
\begin{lstlisting}[language=Haskell,caption=Signature of the \texttt{Const} and \texttt{Simple} data constructors.,numbers=left,frame=bt,firstnumber=5]
Const  :: (Serializable b) 
       => CH.Static (SerializableDict b)
       -> CH.Closure (BasicProcess b)
       -> Process a b

Simple :: (Serializable b)
       => CH.Static (SerializableDict b)
       -> (a -> CH.Closure (BasicProcess b))
       -> Process a b
\end{lstlisting}

We extend our model for processes and add the \texttt{Local}\index{Data constructor!Local} data constructor. \texttt{Local} is a simple decorator\index{Decorator} \cite{Gamma:1995:DPE:186897}, with the purpose to give an indication to the process interpreter that the wrapped process should be executed locally. Typically, the reason for this arises from the expectation that serialising a closure, sending it to a remote node, executing it there and obtaining the result is more expensive\footnote{Expensive in terms of the necessary amount of time to run the process.} than executing the respective process locally. Since there is no general approach to estimate the necessary amount of time to run a process, we decide to equip the user with a tool to force local execution of a process and burden him with the obligation to make appropriate use of it.
\begin{lstlisting}[language=Haskell,caption=Signature of the additional \texttt{Local} data constructor.,numbers=left,frame=bt,firstnumber=14]
Local :: Process a b
      -> Process a b
\end{lstlisting}

The signature of the rest of the data constructors, i.e. \texttt{Sequence}, \texttt{Parallel}, \texttt{Multilel} and \texttt{Repetition}, remain unchanged.

\subsection{Architecture of the infrastructure\index{Architecture}}
\label{chp:infrastructure}
For the interpretation of process structures in a distributed system, we need some kind of infrastructure and management of involved nodes, so we can delegate process execution to them. In this chapter, we describe the architecture and functioning of our system.
\begin{figure}[h!]
  \centering
  \begin{tikzpicture} [every node/.style={fill=black, circle, inner sep = 2pt}]
    \pgftext{
      \includegraphics[width=\textwidth]{img/architecture.pdf}
    }
    \node at (-4.25, 0.25) {\tiny \color{white} 1};
    \node at (-3.75, 0.25) {\tiny \color{white} 2};
    \node at ( 1.50, 0.25) {\tiny \color{white} 3};
    \node at (-0.10, 1.75) {\tiny \color{white} 4};
    \node at (-3.25, 0.25) {\tiny \color{white} 5};
    \node at ( 2.00, 0.25) {\tiny \color{white} 6};
    \node at (-0.10, 1.25) {\tiny \color{white} 7};
    \node at (-2.75, 0.25) {\tiny \color{white} 8};
    \node at (-0.10, 0.75) {\tiny \color{white} 9};
    \node at (-2.30,-1.25) {\tiny \color{white} 10};
    \node at (-0.80,-1.25) {\tiny \color{white} 10};
    \node at ( 2.50,-1.25) {\tiny \color{white} 10};
    \node at ( 1.00,-3.60) {\tiny \color{white} 11};
    \node at ( 6.10, 0.65) {\tiny \color{white} 12};
    \node at ( 2.50, 0.30) {\tiny \color{white} 13};
    \node at ( 3.10, 0.30) {\tiny \color{white} 14};
    \node at ( 3.70, 0.30) {\tiny \color{white} 15};
  \end{tikzpicture}
  \caption{Architecture of the infrastructure }
  \label{fig:architecture}
\end{figure}

The architecture of our system's infrastructure is intentionally kept simple as shown in \figref{fig:architecture}. It involves a designated \textbf{master}\index{Node!master} node and a collection of \textbf{worker}\index{Node!worker} nodes. In addition to that, we need an interface\index{Interface} through which we can insert processes into the system. The easiest way to do so is a command line interface, however, a web interface would be much more convenient and easier to use. We employ Yesod\footnote{Yesod is a Haskell web framework. More information about Yesod can be found at \url{http://www.yesodweb.com/} and \url{https://github.com/yesodweb/yesod}}\index{Yesod} to build a RESTful\index{REST} web interface.

When a client\index{Client} wants to submit a request\index{Request} \figannotation{1}, it has to make use of the web interface and supply a JSON\footnote{JavaScript Object Notation \url{http://www.json.org/}}\index{JSON} document in an appropriate format. In a productive environment, there would be a parser that reads a submitted JSON document, generates a corresponding process structure from its content and passes it on to the process interpreter if the document is valid. For simplicity, we assume for now that a request equals a valid process description and can be interpreted.

The master\index{Node!master} node is responsible for handling client\index{Client} requests, logging information related to the received requests and managing connected nodes. When a request is received \figannotation{2}, the master spawns a new \textsf{Cloud Haskell} \texttt{Process} that runs the \textsc{Hive} process interpreter\index{Interpreter} and passes the request to the new process where it is interpreted \figannotation{3}. At the same time, the master assigns a ticket id to the request, logs the request together with its id to a database\footnote{For this purpose, we make use the \texttt{acid-state} library. \texttt{acid-state} allows to save Haskell values into a file-based database if their type has an instance of \texttt{SafeCopy}. Further information on \texttt{acid-state} can be found at \url{http://acid-state.seize.it/} and \url{https://github.com/acid-state/acid-state}} \figannotation{4} and reports the ticket id to the client \figannotation{5}. When the interpreter finishes interpreting the process structure from the request, it reports the result to the master \figannotation{6}. The master then updates the database \figannotation{7} by logging the result to the database and linking it to the according id. At any time, a client can try to retrieve the result to its request by supplying the request's ticket id to the master through the interface \figannotation{8}. The master then tries to read the result from the database \figannotation{9}, but only replies to the client's satisfaction after the interpretation of its request has been finished and logged to the database.

Worker nodes\index{Node!worker} are kept very simple. When started and supplied with the master's address, they report their availability to the master \figannotation{10} and wait for further instruction.

The \textsc{Hive} process interpreter\index{Interpreter}, which we discuss in full detail in \chpref{chp:interpreter}, is run in a new process by the master for every client request\index{Request} that is received. The interpreter inspects the structure of the received request, transforms it into a \textsc{Hive} process structure and distributes the incorporated sub-processes\index{Sub-process} to worker\index{Node!worker} nodes \figannotation{11} or executes them locally \figannotation{12} if they have been marked for local execution using the \texttt{Local} wrapper. For every basic sub-process, the interpreter asks the master for a worker node \figannotation{13} where it can run the sub-process. For such a request, the master fetches a worker node from a FIFO queue and returns it to the interpreter \figannotation{14}. After a sub-process has finished execution on a worker node, the interpreter returns the respective worker node to the master \figannotation{15} for future allocation to other interpreters. It would very well be possible to employ a more sophisticated scheduling\index{Scheduling} algorithm, but the combination of a FIFO\index{FIFO} queue and a work stealing\index{Work stealing} \cite{} alike behaviour of the process interpreter is fairly efficient and easy to implement. At this time, our primary goal is not to achieve the best possible load balancing or employ the most efficient technique for queueing worker nodes, however this leaves room for future improvement.


\subsection{Process interpreter\index{Process interpreter}\index{Interpreter}}
\label{chp:interpreter}
With the involvement of a distributed system, we need to change the implementation of the process interpreter. Where there was only local parallelisation in the \texttt{IO} monad involved before, we have to adapt process interpretation to the specifics of the \textsf{Cloud Haskell} \texttt{Process} monad. This particularly involves remote execution of processes.

When the master\index{Node!master} node receives a client\index{Client} request\index{Request}, it spawns a process interpreter on its local node and passes the process structure from the request to the interpreter. The interpreter then inspects the process structure and distributes the incorporated sub-processes\index{Sub-process} to connected worker nodes accordingly. To do so, the interpreter asks the master node for an available worker node for every sub-process that has to be executed\footnote{Except for processes wrapped into a \texttt{Local} process, of course.}. We represent the master\index{Node!master} node by a \texttt{ProcessId} that is wrapped into a value of type \texttt{Master}.
\begin{lstlisting}[language=Haskell,caption=Data type for the address of a master node.,numbers=left,frame=bt]
newtype Master = Master ProcessId
\end{lstlisting}

We have to adapt the signature of \texttt{runProcess} and introduce an additional parameter, as shown in \lstref{lst:interpreter_signature}. As before, \texttt{runProcess} takes a process of type \texttt{Process a b}, an argument for the process of type \texttt{a} and returns a value of type \texttt{b}. Furthermore, for process distribution, \texttt{runProcess} requires a value of type \texttt{Master}, so it can request worker nodes to run processes on.
\begin{lstlisting}[language=Haskell,caption=Signature of the process interpreter.,label=lst:interpreter_signature,numbers=left,frame=bt,firstnumber=2]
runProcess :: Master -> Process a b -> a -> CH.Process b
\end{lstlisting}

For the interpretation of a \texttt{Const}\index{Process interpreter!Const} process, we take the \texttt{sDict} value of type \texttt{SerializableDict b} and the \texttt{closure} of type \texttt{CH.Closure (BasicProcess b)} from the \texttt{Const} process and create a new \texttt{Simple} process out of that. For that we can simply keep the \texttt{sDict} as it is but need to turn the \texttt{closure} into a closure generator\index{Closure!generator}, i.e. a function that takes a value of type \texttt{a} and generates a closure\index{Closure} from it. Since we're dealing with a \texttt{Const} process which simply discards its input value and always behaves the same, we have to respect this when creating the closure generator for the new \texttt{Simple} process. This can be done by using Haskell's standard function \texttt{const} which takes two values, discards the second one and always returns the first one. Then, all we have to do is pass the new \texttt{Simple} process into the process interpreter.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter of \texttt{Const} processes.,label=lst:runprocess_const,numbers=left,frame=bt,firstnumber=3]
runProcess master (Const sDict closure) x =
  runProcess master (Simple sDict (const closure)) x
\end{lstlisting}

When encountering a \texttt{Simple}\index{Process interpreter!Simple} process in a process structure, the process interpreter asks the master node for an available worker node where it can run this basic process, as shown in line 2 of \lstref{lst:runprocess_simple}. This operation blocks until the master node is able to satisfy the request for an available worker node and supplies it to the interpreter. The interpreter then uses the closure generator \texttt{closureGen}, which has type \texttt{a $\to$ CH.Closure (BasicProcess b)}, to generate a closure that is then serialised and sent to the available worker node for execution. This operation blocks the interpreter until execution of the remotely spawned process on the worker node has terminated and a result value is obtained. The interpreter then returns the worker node to the master and the remotely calculated result to its caller.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Simple} processes.,label=lst:runprocess_simple,numbers=left,frame=bt,firstnumber=5]
runProcess master (Simple sDict closureGen) x = do
  node <- getNode master =<< getSelfPid
  res  <- call sDict node (closureGen x)
  returnNode master node
  return res
\end{lstlisting}

Processes that are wrapped into a \texttt{Local}\index{Process interpreter!Local} wrapper are supposed to be executed locally on the same node by the process interpreter instead of being distributed to remote nodes, this also includes sub-processes. To accomplish this behaviour, we apply a little trick: we create a fake master that always returns the local node when asked for an available worker node and we're discarding the real master node that is passed as first argument to \texttt{runProcess}, as shown in \lstref{lst:runprocess_local}. We use the fake master to pass it to a recursive call of \texttt{runProcess} that interprets the process that is wrapped into the \texttt{Local} wrapper at hand. Thereby, we're making sure that for the interpretation of the tree of sub-process the fake master is used and all sub-processes are interpreted locally. When the interpretation of the process structure has been finished, we terminate the fake master we had created before.
\begin{lstlisting}[language=Haskell,caption=Implementation of the interpreter for \texttt{Local} processes.,label=lst:runprocess_local,numbers=left,frame=bt]
runProcess _ (Local p) x = do
  fakeMaster <- getFakeMaster =<< getSelfPid
  res <- runProcess fakeMaster p x
  terminateMaster fakeMaster
  return res
\end{lstlisting}

The adaptations that have to be made for the remaining process types, i.e. \texttt{Choice}, \texttt{Sequence}, \texttt{Parallel}, \texttt{Multilel} and \texttt{Repetition} are minimal. They involve adding the necessary \texttt{master} parameter and adaptations specific to the \textsf{Cloud Haskell} \texttt{Process} monad, i.e. using \texttt{spawnLocal} instead of \texttt{forkIO} and lifting action on MVars from the \texttt{IO} monad into the \texttt{Process} monad using \texttt{liftIO}. The implementation can be found in \appref{app:distributed_split_slice}.

\index{Hive process interpreter!Choice}

\subsection{Handling a request}
Now that we have taken a look at how our system is structured and how the \textsc{Hive} process interpreter works, we want to take a look at what happens in the system when a client submits a request.

For this purpose, assume that the master node is already running with a set of worker nodes connected. The client has knowledge about the address of the master and the ability to directly send requests to it. \Figref{fig:request_handling} shows the behaviour of the system that follows when a client submits a request. Note that we do not explicitly show all the worker nodes involved but symbolically combine them as \enquote{Worker}.

\begin{figure}[h!]
  \centering
  \input{img/request_handling.tex}
  \caption{Behaviour of the system when a request is submitted by a client.}
  \label{fig:request_handling}
\end{figure}

\begin{itemize}
  \item [\figannotation{1}] The client submits a request to the master node.
  \item [\figannotation{2}] The master node replies with the assigned ticket to the client.
  \item [\figannotation{3}] The master node spawns a new process interpreter on its own node and passes the request to it.
  \item [\figannotation{4}] The interpreter asks the master node for worker nodes for the purpose of spawning processes on them.
  \item [\figannotation{5}] The master node replies with available worker nodes to the interpreter. If there are no worker nodes available, the master waits with this reply until worker nodes become available.
  \item [\figannotation{6}] The interpreter spawns processes on the worker nodes.
  \item [\figannotation{7}] Once a process on a worker node has calculated its result, it sends it back to the interpreter and terminates.
  \item [\figannotation{8}] The client supplies the previously received ticket number to the master, hoping the request has been processes completely.
  \item [\figannotation{9}] The master tells the client that its request hasn't been completely processes yet. There is no solution to the client's problem available yet.
  \item [\figannotation{10}] After execution of processes on the worker nodes, the interpreter returns them to the master node for further allocation to other interpreters. Note that steps \figannotation{4} to \figannotation{10} can, and most likely will, happen multiple times while processing a request.
  \item [\figannotation{11}] Once the interpreter has finished processing the client's request, it submits the solution to the problem to the master.
  \item [\figannotation{12}] The client supplies the ticket to the master again.
  \item [\figannotation{13}] The master responds to the client with the calculated solution.
\end{itemize}

\section{Example: arithmetic expressions - hello world for interpreters\index{Example}}
\label{chp:example}
After discussing the process model and the interpreter that takes care of processes interpretation, we give an example of how to apply our calculus. To do so, we develop the typical hello world program for interpreters, i.e. an interpreter\index{Interpreter} for arithmetic expressions\index{Arithmetic expression} with parallel evaluation of sub-expressions. Except for some imports of standard \textsf{Haskell} libraries, this example is self-contained. It is intentionally kept very simple and the necessity for a parallelised interpreter for arithmetic expressions is certainly not given. However, an interpreter for arithmetic expressions is fairly simple and thus well suited to illustrate how to apply the our process calculus to it.

First, we need an appropriate data type to represent arithmetic expressions. An arithmetic expression \texttt{Expr} is either a value \texttt{Val} containing an \texttt{Int} or a combination of two arithmetic expressions. The combinators we want to support are addition \texttt{Add}, subtraction \texttt{Sub}, multiplication \texttt{Mul} and division \texttt{Div}. The relevant data type together with the data constructors can be found in \lstref{lst:arith_model} 
\begin{lstlisting}[language=Haskell, caption=Data model for the representation of arithmetic expressions., label=lst:arith_model, numbers=left, frame=bt]
data Expr = Val Int
          | Add Expr Expr
          | Sub Expr Expr
          | Mul Expr Expr
          | Div Expr Expr
\end{lstlisting}

When we use an interpreter to interpret an arithmetic expression, we expect it to return a value of type \texttt{Int}. The implementation of such an interpreter \texttt{eval} can be found in \lstref{lst:arith_eval} and is straightforward. The interpreted value of a \texttt{Val} expression is simply its wrapped \texttt{Int} value. The interpreted value of a more complicated expression is given by recursively interpreting the involved sub-expressions and combining the results with the appropriate operator, i.e. \texttt{+} for \texttt{Add}, \texttt{-} for \texttt{Sub}, \texttt{*} for \texttt{Mul} and \texttt{div} for \texttt{Div}.
\begin{lstlisting}[language=Haskell, caption=Implementation of an interpreter for arithmetic expressions of type \texttt{Expr}., label=lst:arith_eval, numbers=left, frame=bt, firstnumber=6]
eval :: Expr -> Int
eval (Val i) = i
eval (Add x y) = eval x + eval y
eval (Sub x y) = eval x - eval y
eval (Mul x y) = eval x * eval y
eval (Div x y) = eval x `div` eval y
\end{lstlisting}

Now, if we want to make use of the process interpreter \texttt{runProcess} to parallelise the interpretation of arithmetic expressions, we need to define a transformation from arithmetic expressions \texttt{Expr} to processes \texttt{Process a b}. Arithmetic expressions of type \texttt{Expr} do not take any parameter, they are fully defined by the values incorporated into \texttt{Val} values and yield a value of type \texttt{Int} when interpreted and hence need to be mapped to processes of type \texttt{Process () Int}.

For the representation of arithmetic expressions that have the form of \texttt{Val}, we implement a function \texttt{val} that returns a basic processes that can be turned into a process by wrapping it into a \texttt{Const} wrapper. The implementation of \texttt{val} can be found in \lstref{lst:arith_val}.
\begin{lstlisting}[language=Haskell, caption=A function that generates basic processes for the representation of \texttt{Val} nodes., label=lst:arith_val, numbers=left, frame=bt, firstnumber=12]
val :: Int -> BasicProcess Int
val i = return i
\end{lstlisting}

Arithmetic expressions of the form \texttt{Add}, \texttt{Sub}, \texttt{Mul} and \texttt{Div} can be interpreted in parallel since their sub-expressions are independent from each other. We can exploit \texttt{Parallel} processes to interpret the sub-expressions in parallel. To do so, we need to supply combinator processes for the \texttt{Parallel} processes, so the obtained results from the sub-processes can be combined into a single value. The implementation of the relevant combinator processes can be found in \lstref{lst:arith_combinators}, they all have the same type \texttt{(Int, Int) $\to$ BasicProcess Int}.
\begin{lstlisting}[language=Haskell, caption=Basic processes for the combination of results from processes that have been executed in parallel., label=lst:arith_combinators,numbers=left, frame=bt, firstnumber=14]
val :: Int -> Process () Int
val = Const . return 

add :: Process (Int, Int) Int
add = Simple (return . uncurry (+))

subtract :: Process (Int, Int) Int
subtract = Simple (return . uncurry (-))

multiply :: Process (Int, Int) Int
multiply = Simple (return . uncurry (*))

divide :: Process (Int, Int) Int
divide = Simple (return . uncurry div)
\end{lstlisting}

All prerequisites are met now and we can start with the transformations from \texttt{Expr} to \texttt{Process () Int}, which we implement an interpreter for. It can be found in \lstref{lst:arith_transformation}: \texttt{interpret} maps expressions of the form \texttt{Val} to a basic process that simply returns a wrapped value and maps composed expressions to composed processes using the \texttt{Parallel} type constructor. When passing the resulting process to the process interpreter \texttt{runProcess}, it takes care of the parallelisation of the sub-processes of the \texttt{Parallel} nodes.
\begin{lstlisting}[language=Haskell, caption=Transformation from \texttt{Expr} to processes., label=lst:arith_transformation, numbers=left, frame=bt, firstnumber=43]
transform :: Expr -> Process () Int
transform (Val i)   = val i
transform (Add x y) = Parallel add      (interpret x) (interpret y)
transform (Sub x y) = Parallel subtract (interpret x) (interpret y)
transform (Mul x y) = Parallel multiply (interpret x) (interpret y)
transform (Div x y) = Parallel divide   (interpret x) (interpret y)
\end{lstlisting}

In this example we have defined a data structure that can be used to represent arithmetic expressions and implemented a conventional interpreter for this data structure. Then, we have defined a mapping from these arithmetic expressions to processes, and used the process interpreter to parallelise their interpretation. We did not have to explicitly implement parallelisation or thread management of any kind for this, but only have to make appropriate use of the process calculus to represent our computation as a composed process.