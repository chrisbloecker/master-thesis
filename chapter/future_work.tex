\chapter{Future Work}
In this chapter, we discuss open research questions and ideas on implementation improvements.

In the case of ant systems, we have seen that our process calculus can be used to generate a speedup in execution. We are wondering if this is the case for other algorithms as well and how big the achievable speedup for different algorithms is. A related question is whether our distributed implementation is unsuitable for practical application in general or if it shows a different behaviour concerning the speedup in case of other algorithms.

The technical details of how processes receive their input differ between the implementations based on \textsf{Concurrent Haskell} and \textsf{Cloud Haskell}. Where in \textsf{Concurrent Haskell} the process interpreter can simply pass a reference to the data structure to the respective process, in \textsf{Cloud Haskell} this involves serialising data and sending it to a remote node. This has to be done every time a new process is spawned, even for constant data that never changes. Introducing a data distribution layer that takes care of making data available to the nodes where it is needed should help reducing this overhead. The data distribution layer could involve a centralised data store running on the master node. Process interpreters could then submit data to the data store and receive a key that identifies the submitted data. For passing data to a process, only the key associated to the data would be needed. When a process wants to access data based on a key, the data distribution layer would take care of copying the data into a local cache on the relevant worker node, making it available for future use without the need to transmit it over the network again.

In the current implementation of the scheduling strategy for assigning worker nodes to process interpreter, the presence of a single slow computer could slow the whole system down. The master node keeps the available worker nodes in a FIFO queue. This way, every node gets to do some work eventually. However, this also means that situations are likely to occur where a worker node running on a slow computer is assigned, although a worker node running on a faster computer is available. Introducing additional information about worker nodes and employing a more sophisticated scheduling strategy should help to improve the situation.