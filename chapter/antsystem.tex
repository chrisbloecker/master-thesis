\chapter{A real world example}
In this chapter we use the \textsc{Hive} process algebra to find solutions for the travelling salesman problem. Since the travelling salesman problem is considered to be a computationally hard optimisation problem, we don't have extended hopes in finding optimal solutions and therefore apply a meta-heuristic approach to solve it.

\section{The travelling salesman problem}
The travelling salesman problem (or short: \textsc{TSP}) is a graph theoretic optimisation problem. For a graph $G = \left( V, E, \delta \right)$ where $V = \left\{ v_1, v_2, \ldots, v_n \right\}$ is the set of nodes, $E \subseteq V \times V$ is the set of edges, i.e. the connections between the nodes, and $\delta \colon E \to \mathbb{N}$ is a function that assigns a length to every edge, the \textsc{TSP} asks for the \textbf{shortest} round trip through the nodes of $G$.

A round trip is a permutation of the nodes of $G$, hence every node appears exactly once in a round trip. The length of a round trip is given by the sum of the lengths of its pieces, which are defined by $\delta$. Let $i = \left( i_1, i_2, \ldots, i_n \right)$ be a permutation of the natural numbers from $[1, n] \subset \mathbb{N}$. $i$ defines a numbering for the nodes of $G$ and leads to a permutation of them. Let $r_i = \left( v_{i_1}, v_{i_2}, \ldots v_{i_n} \right)$ be the round trip defined by the indexes given by $i$. The length $\phi \left( r_i \right)$ of $r_i$ can be calculated using
\begin{equation}
  \label{eqn:length_roundtrip}
  \phi \left( r_i \right) = \delta \left( \left( v_{i_n}, v_{i_1} \right) \right) + \sum_{j=1}^{n-1} \delta \left( \left( v_{i_j}, v_{i_{j+1}} \right) \right).
\end{equation}
Note that in order to resemble the cyclic nature of a permutation, we have to also add the length of the last node back to the first. This is also necessary to make the path induced by the permutation a round trip.

The \textsc{TSP} is considered to be a computationally hard optimisation problem and belongs to the class of $\mathcal{NP}$-hard problems \cite{Garey:1979:CIG:578533}. This means that there is no known efficient algorithm that finds the solution to an arbitrary instance of the \textsc{TSP}. Furthermore, it is widely believed that no such algorithm can exist at all. Except for some special cases where there are constraints put on the structure of the graph, the only way to find the optimal solution is to check \textbf{all} the possible solutions for optimality. Since for an arbitrary graph the number of possible solutions is exponential in its number of nodes, the approach of exploring \textbf{every} possible solution is impractical even for graphs with reasonably few nodes.

\section{Meta-heuristics}
Meta-heuristics\footnote{Heuristic, from Greek $E\nu\rho\iota\sigma\kappa\omega$: \enquote{find} or \enquote{discover}.} are an approach to tackle the problem that arises with huge sets of possible solutions like in the case of the \textsc{TSP}. The idea of meta-heuristics is to explore the set of possible solutions in a more intelligent way than checking \textbf{every} possible solution for optimality. Meta-heuristics try to make an \enquote{intelligent guess} about which solution might be close to optimal and can be based on different, problem-specific criteria.

One thing different meta-heuristics have in common is a solution finding principle, called \textbf{local search}: first, an initial solution candidate is generated, the meta-heuristic aims to improve this candidate iteratively. Then, based on defined criteria, modifications are made to the solution candidate in the hope of finding a closer to optimal solution. The modifications that are made to the solution candidate yield a set of similar solutions, called the \textbf{local neighbourhood} of the solution candidate. From the local neighbourhood, the closest to optimal candidate gets picked to replace the previous candidate.

When applying this approach, it might happen that in the local neighbourhood of a solution candidate, there is no closer to optimal solution than the candidate itself. In this case, the candidate is considered to be a so called \textbf{local optimum}. A local optimum might be the global optimum, however in general this is highly unlikely. Therefore, if the meta-heuristic runs into such a situation, it must allow to escape local optima and execute steps towards farther from optimal solutions. As a direct consequence, the meta-heuristic must have a way to prevent from directly running back into local optima as this may very well happen in the step after proceeding towards a farther from optimal solution \cite{}. A common approach to achieve this is the use of a so called \textbf{tabu list}: a tabu list keeps track of the most recently visited solutions candidates \cite{}. As long as a candidate is in the tabu list, it must not be chosen again. Unfortunately this does not entirely eliminate the possibility of running into the same solution again. A solution space might have cycles of 

The term meta-heuristic is chosen because it involved heuristics to guess solutions and is applicable to a wide field of problems, so we have a recipe of how to build a heuristic algorithm to solve arbitrary problems, as far as we have the possibility to define when solutions are similar.

\section{Artificial ant systems}
