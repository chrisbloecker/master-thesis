\chapter{A real world example\index{Example}}
In this chapter we use the \textsc{Hive} process algebra to build a distributed system that finds solutions for the travelling salesman problem\index{Travelling Salesman Problem}. Since the travelling salesman problem is considered to be a computationally hard optimisation problem, we don't have extended hopes in finding optimal solutions and therefore apply a meta-heuristic\index{Meta-heuristic} approach to solve it.

\section{The travelling salesman problem\index{Travelling Salesman Problem}}
\label{chp:tsp}
The travelling salesman problem (or short: \textsc{TSP}\index{TSP}) is a graph theoretic\index{Graph theory} optimisation problem. For a graph $G = \left( V, E, \delta \right)$ where $V = \left\{ v_1, v_2, \ldots, v_n \right\}$ is the set of nodes, $E \subseteq V \times V$ is the set of edges, i.e. the connections between the nodes, and $\delta \colon E \to \mathbb{N}$ is a function that assigns a length to every edge, the \textsc{TSP} asks for the \textbf{shortest} round trip through the nodes of $G$.

\nomenclature{$G$}{A graph}
\nomenclature{$V$}{Set of nodes of a graph}
\nomenclature{$E$}{Set of edges of a graph}
\nomenclature{$\delta$}{Labelling function for graph edges}
\nomenclature{TSP}{Travelling Salesman Problem}


A round trip\index{Round trip} is a permutation\index{Permutation} of the nodes of $G$, hence every node appears exactly once in a round trip. The length of a round trip is given by the sum of the lengths of its pieces, which are defined by $\delta$. Let $i = \left( i_1, i_2, \ldots, i_n \right)$ be a permutation of the natural numbers from $[1, n] \subset \mathbb{N}$. $i$ defines a numbering for the nodes of $G$ and leads to a permutation of them. Let $r_i = \left( v_{i_1}, v_{i_2}, \ldots v_{i_n} \right)$ be the round trip defined by the indexes given by $i$. The length $\phi \left( r_i \right)$ of $r_i$ can be calculated using
\begin{equation}
  \label{eqn:length_roundtrip}
  \phi \left( r_i \right) = \delta \left( \left( v_{i_n}, v_{i_1} \right) \right) + \sum_{j=1}^{n-1} \delta \left( \left( v_{i_j}, v_{i_{j+1}} \right) \right).
\end{equation}
Note that in order to resemble the cyclic nature of a permutation, we have to also add the length of the edge from the last node back to the first node. This is also necessary to make the path induced by the permutation a round trip.

The \textsc{TSP} is considered to be a computationally hard optimisation\index{Optimisation problem} problem and belongs to the class of $\mathcal{NP}$-hard problems \cite{Garey:1979:CIG:578533}. This means that there is no known efficient algorithm\index{Algorithm} that finds the solution to an arbitrary instance of the \textsc{TSP}. Furthermore, it is widely believed that no such algorithm can exist at all. Except for some special cases where there are constraints put on the structure of the graph, the only way to find the optimal solution is to check \textbf{all} possible solutions for optimality \cite{}. Since for an arbitrary graph the number of possible solutions is exponential in its number of nodes, the approach of exploring \textbf{every} possible solution is impractical even for graphs with reasonably few nodes.

A real world application of the \textsc{TSP} is just as the name suggests a scenario where there is a travelling salesman who has to visit a number of houses each exactly once to sell his goods and then return home. It is naturally in his interest to choose the shortest way in order to minimise travelling costs and/or time. Other real world application include e.g. the problem of determining in which order to drill holes for mining operations in order to minimise moving time for the drill or ToDo: other example.

\section{Meta-heuristics\index{Meta-heuristic}}
\label{chp:meta_heuristics}
Meta-heuristics\footnote{Heuristic, from Greek $E\nu\rho\iota\sigma\kappa\omega$: \enquote{find} or \enquote{discover}.} are an approach to tackle the problem that arises with huge sets of possible solutions like in the case of the \textsc{TSP}. The idea of meta-heuristics is to explore the set of possible solutions in a more intelligent way than checking \textbf{every} possible solution for optimality. Meta-heuristics try to make an \enquote{intelligent guess} about which solution might be close to optimal and can be based on different, problem-specific criteria. As a direct consequence of \enquote{intelligent guessing}, it is very well possible to miss the optimal solution.

One thing different meta-heuristics have in common is a solution finding principle called \textbf{local search}\index{Local search}. In \textbf{local search}, first an initial solution candidate\index{Candidate} is generated using a suitable construction algorithm\index{Algorithm}. The meta-heuristic aims to improve this candidate iteratively: based on defined criteria, modifications are made to the solution candidate in the hope of finding a closer to optimal solution. The modifications yield a set of solutions similar to the solution candidate, called its \textbf{local neighbourhood}\index{Neighbourhood}. From the local neighbourhood, the closest to optimal solution is chosen to replace the previous solution candidate.

When applying this approach, it might happen that in the local neighbourhood of a solution candidate, there is no closer to optimal solution than the candidate itself. In this case, the candidate is considered to be a so called \textbf{local optimum}. A local optimum\index{Optimum!local} might be the global optimum\index{Optimum!global}, however in general this is highly unlikely. Therefore, if the meta-heuristic runs into such a situation, it must allow to escape local optima and execute steps towards farther from optimal solutions. As a direct consequence, the meta-heuristic must have a way to prevent from directly running back into local optima as this may very well happen in the step after proceeding towards a farther from optimal solution \cite{}. A common approach to achieve this is the use of a so called \textbf{tabu list}\index{Tabu list} a tabu list keeps track of the most recently visited solutions candidates \cite{}. As long as a candidate is in the tabu list, it must not be chosen again. Unfortunately this does not entirely eliminate the possibility of running into the same solution again. The solution space might have cycles of higher length than the length of the tabu list, allowing to go back to an undesirable solution.

The term meta-heuristic is chosen because it involves heuristics to guess solutions and is applicable to a wide field of problems. We thereby have a recipe of how to build a heuristic algorithm to solve arbitrary problems, as long as we can define the local neighbourhood of a solution.

\section{Artificial ant systems\index{Ant system}}
\label{chp:ant_system}
Artificial ant systems are a meta-heuristic\index{Meta-heuristic} approach that can be applied to a big variety of computationally hard optimisation problems that can be represented as a graph\index{Graph}, like the \textsc{TSP}. An overview about different types of ant systems and example applications can be found in \cite{Dorigo:2004:ACO:975277}. Here, we only briefly discuss one form of ant systems and use the \textsc{Hive} process algebra to implement a prototype\index{Prototype} of an ant system that finds approximate solutions for instances of the \textsc{TSP}\index{TSP}. Without further mentioning, \cite{Dorigo:2004:ACO:975277} serves us as a reference throughout this chapter.

Artificial ant systems are inspired by the swarm behaviour of ants from ant colonies in nature. When in search for new food sources, natural ants explore their environment by performing random walks. Once they discover a new food source, they return to their colony and mark the way to the food source with so called \textit{pheromones}\index{Pheromone}. These pheromones can be sensed by other ants to guide their way to the food source. When an ant encounters a pheromone trail, it makes a decision between following it or continuing exploring its environment randomly. The probability of following a trail gets higher as it gets covered more densely with pheromones. If the ant decides to follow the pheromone trail, it continues up to the food source, picks up some food and carries it back to its colony. On the way back, it reinforces the pheromone trail by depositing additional pheromones. Over time, more and more ants follow the pheromone trail and an ant trail emerges. A natural phenomenon, called \textit{evaporation}\index{Evaporation}, counters the built-up of trails: a small fraction of pheromones disappears continuously, so the trail vanishes eventually if there are no ants left reinforcing it.

Artificial ant systems resemble the described situation from nature and employ a graph\index{Graph} to do so. Depending on the problem at hand, one node of the graph is selected to represent the ant colony. At this special node, ants are generated to perform an exploration of the graph and construct a solution to the problem represented by the graph. In the case of the \textsc{TSP}\index{TSP}, a valid solution is one that represents a permutation of the graph's nodes and it does not matter which node is selected to represent the colony since every node has to appear in the solution exactly once. An ant starts its round trip through the graph at the colony and then selects one of the unvisited nodes and proceeds to it. It continues doing so until it has visited all of the nodes, then it returns back to the colony.

When construction a round trip for a graph $G = \left( V, E, \delta \right)$, ants are guided by two different values. On the one hand, they use a heuristic value, based on the value of an edge to estimate its desirability: for an edge $e \in E$, its heuristic value is given by
\begin{equation}
  \label{eqn:eta}
  \eta \left( e \right) = \frac{1}{\delta \left( e \right)}.
\end{equation}
An edge with a higher value, i.e. a longer edge, will have a lower heuristic value than an edge with a low value, i.e. a short edge. For ants, higher heuristic values are desirable. On the other hand, pheromone trails are used. In the beginning, an initial pheromone concentration $\tau_0$ is deposited on every edge of the graph. As the ant system preforms its work, the pheromone values will be updated and changed over time.

In each step, an ant picks the next node to visit non-deterministically based on a visiting probability obtained from a combination of heuristic and pheromone values. Let $N \left( i \right)$ be the set of unvisited neighbour nodes for an ant that is currently at node $i$. The probability of visiting node $j$ next is then
\begin{equation}
  \label{eqn:probability}
  p_j = \frac{\tau_{i,j}^\alpha \eta_{i,j}^\beta}{\sum_{j \in N} \tau_{i,j}^\alpha \eta_{i,j}^\beta}.
\end{equation}
where $\alpha$ and $\beta$ are values to weigh the influence of $\tau$ and $\eta$. $\tau_{i,j}$ and $\eta_{i,j}$ are just a short hand notation for the pheromone concentration on the edge between nodes $i$ and $j$, and the heuristic value of that edge respectively. A random value $r \in \left[0, 1 \right]$ is then generated to choose a node. However, to save runtime, we apply the approach from \cite{Bloecker} and only calculate the nominators. Then we sum them up and generate a random value $r' \in \left[0, \sum_{j \in N} p_j \right]$. This leaves the result unchanged.

As a direct consequence of how ants construct their way through the graph, they need a complete graph to operate on, otherwise they might run into situations where they get stuck in one node because there are no more reachable, unvisited nodes left from where they are. Fortunately, every graph can be extended to be a complete graph without altering the optimal solution to the \textsc{TSP}: Let $G = \left( V, E, \delta \right)$ be a graph. For every edge $\overline{e}$ that does not exist in the graph, i.e. $\overline{e} \in \left( V \times V \right) \setminus E$, we introduce a new edge and assign a suitably large value $\delta \left( \overline{e} \right)$ to it, e.g. $\delta \left( \overline{e} \right) = 1 + \sum_{e \in E} \delta \left( e \right)$. We make sure that none of the added edges can possibly be part of an optimal solution by assigning a value to them that is guaranteed to be higher than the value of the optimal solution. For other problems, the process of adapting the graph to a suitable form may look differently.

When all ants have finished the construction of a round trip, each of them deposits new pheromones on all the edges it has used for the construction of its solution. The longer the round trip is, the less desirable it is and hence, the less pheromones should be added by that ant. Let $s_i$ be the solution candidate, i.e. the round trip, that has been constructed by ant $i$ and let $c_s$ be its costs, induced by $\delta$ and summation over the used edges in $s_i$. Then ant $i$ is allowed to deposit additional pheromones of $\tau_i^+ = \frac{1}{c}$ on every used edge and $\tau_i^+ = 0$ on unused edges. Note that there are various other ways to handle pheromone updates, many of which can be found in \cite{Dorigo:2004:ACO:975277}.

After ants have deposited new pheromones, evaporation takes place on all edges. An evaporation factor, $\rho$, specifies which fraction of the pheromones survives evaporation:
\begin{equation}
  \label{eqn:evaporation}
  \tau_{i,j} \leftarrow \left( 1 - \tau \right) \tau_{i,j}.
\end{equation}
However, \cite{Bloecker} point out that in some cases evaporation might have stronger effects than deposition of new pheromones. This would be the case if solutions with high values are involved, which is not a rare case in real world scenarios. If the value $c_i$ of a solution $i$ is high, then the value of new pheromones, $\frac{1}{c_i}$ is low and possibly close to $0$, which might practically be effectless. To compensate for that, \cite{Bloecker} suggest to use a value of $\tau_i^+ = \frac{c_{best}}{c_i}$ instead, where $c_{best}$ is the value of the best solution found so far, which has to be kept track of. This way, additional pheromones for solutions that have a value close to the best known solution, will have a value close to 1.

Ant systems perform their work in three phases:
\begin{enumerate}
  \item initialisation \\
    initial pheromones are deposited on all edges of the graph
  \item construction \\
    a predefined number of ants is created, each of which constructs a solution candidate
  \item update \\
    ants deposit new pheromones, evaporation takes place
\end{enumerate}
Phase (1) takes place exactly once. Phases (2) and (3) take place in turn until either a predefined maximum execution time is reached or a solution of sufficient quality has been found. Other criteria to abort the computation can be applied as well.

\section{A prototype of an ant system using \textsc{Hive}}
Having introduced the principles of ant systems, we use the \textsc{Hive} process algebra to implement a distributed ant system. Most notably, we do so without specifying any explicit communication between processes, illustrating that \textsc{Hive} fulfils its purpose.

We begin by modelling the configuration for the ant system, including the previously mentioned necessary parameters, the data model for a configuration can be found in \lstref{lst:ant_conf}. The actual problem input is given by \texttt{graph}, \texttt{ants} specifies how many ants should be concurrently used to explore solutions and \texttt{alpha} and \texttt{beta} are used by the ants when exploring solutions (cf. \chpref{chp:ant_system}). The configuration also includes the current pheromones as well as the best solution to the problem that has been found so far, both kept up to date by the ant system. Furthermore, it must be possible to send the configuration over the network to ants running on other nodes in a distributed system. Therefore we need to add instances for the type classes \texttt{Generic}, \texttt{Typeable} and \texttt{Binary}.

\begin{lstlisting}[language=Haskell,frame=tb,numbers=left,label=lst:ant_conf,caption=Configuration for the ant system.]
data Conf = Conf { graph      :: Graph Int
                 , pheromones :: !Pheromones
                 , path       :: !Path
                 , ants       :: Int
                 , iter       :: Int
                 , alpha      :: Double
                 , beta       :: Double
                 }
  deriving (Generic, Typeable)

instance Binary Conf where
\end{lstlisting}

Next we define the basic \textsf{Cloud Haskell} processes which we wrap into \textsc{Hive} processes later. The basic \textsf{Cloud Haskell} processes can be found in \lstref{lst:ant_basic}, but note that we don't show the implementation details of these processes here, they can be found in \appref{app:ant_system_processes}. For now, only the existence and how we work with them is important.

The \texttt{ant} process resembles the work of an ant that is supplied with a configuration and return exactly one solution candidate to the presented problem.

The \texttt{combinePaths} process receives a configuration and a list of candidate solutions and returns a new configuration. To do so, it inspects the list of solution candidates and updates both the pheromone concentrations for the graph stored in the configuration and the best known solution so far.

The \texttt{extractSolution} process is fairly simple and doesn't do anything else than extracting the best known solution from the configuration it is supplied with.

\begin{lstlisting}[language=Haskell,frame=tb,numbers=left,firstnumber=12,label=lst:ant_basic,caption=Basic \textsf{Cloud Haskell} processes.]
ant :: Conf -> CH.Process Path
ant conf = ...

combinePaths :: (Conf, [Path]) -> CH.Process Conf
combinePaths (conf, ps) = ...

extractSolution :: Conf -> CH.Process Path
extractSolution = return . path
\end{lstlisting}

As discussed in \chpref{chp:cloud_haskell}, we need to provide a \texttt{SerializableDict} for every data type that we want to be able to send over the network to another process. In this case, we need to do this for \texttt{Path} and \texttt{Conf} since values of these types are returned by remotely run processes. As a brief reminder: a \texttt{SerializableDict} is basically just an explicit type tag and doesn't involve anything more than using a type constructor, as can be seen in \lstref{lst:ant_dicts}.

\begin{lstlisting}[language=Haskell,frame=tb,numbers=left,firstnumber=20,label=lst:ant_dicts,caption=Dictionaries for serialisation.]
pathDict :: SerializableDict Path
pathDict = SerializableDict

confDict :: SerializableDict Conf
confDict = SerializableDict
\end{lstlisting}

In order to make the basic \textsf{Cloud Haskell} processes and the dictionaries remotely usable, we have to pass their names to \texttt{remotable}, as shown in \lstref{lst:ant_remotable}. \texttt{remotable} creates the closures for the given functions, their decoders and meta information and adds the information to a table of functions that can be used remotely.

\begin{lstlisting}[language=Haskell,frame=tb,numbers=left,firstnumber=25,label=lst:ant_remotable,caption=Making processes and dictionaries remotable.]
remotable [ 'ant
          , 'combinePaths
          , 'extractSolution
          , 'pathDict
          , 'confDict
          ]
\end{lstlisting}

We can now model the basic \textsc{Hive} processes and make use of the previously defined basic \textsf{Cloud Haskell} processes to wrap them into them. The basic \textsc{Hive} processes can be found in \lstref{lst:ant_hive}.

\texttt{antP} defines a \textsf{Hive} process that takes a configuration and returns a \texttt{Path}, which is immediately obvious from its signature. To create this process, we take use \textsf{Template Haskell} to generate a closure from the \texttt{ant} process and a static value of the according dictionary and wrap them using the \texttt{Simple} constructor. To create the \texttt{combinePathsP} and \texttt{extractSolutionP} processes, we proceed analogously: we use \textsf{Template Haskell} to generate the closures and matching dictionaries and wrap them using the \texttt{Simple} constructor from the \textsc{Hive} process algebra. Note that we explicitly model the type of the input and output values of \textsc{Hive} processes in the function signatures.

\begin{lstlisting}[language=Haskell,frame=tb,numbers=left,firstnumber=31,label=lst:ant_hive,caption={\textsc{Hive} processes, built on top of basic \textsf{Cloud Haskell} processes.}]
antP :: Process Conf Path
antP = Simple $(mkStatic 'pathDict)
              $(mkClosure 'ant)

combinePathsP :: Process (Conf, [Path]) Conf
combinePathsP = Simple $(mkStatic 'confDict)
                       $(mkClosure 'combinePaths)

extractSolutionP :: Process Conf Path
extractSolutionP = Simple $(mkStatic 'pathDict)
                          $(mkClosure 'extractSolution)
\end{lstlisting}

Now we have all the pieces prepared we need to put together an ant system: the basic \textsc{Hive} processes (\lstref{lst:ant_hive}), the data model (\lstref{lst:ant_conf}) and the \textsc{Hive} process combinators. We implement a function \texttt{interpret} that takes a configuration for an ant system and generates a \textsc{Hive} process from it that performs the work of the ant system. The implementation of \texttt{interpret} can be found in \lstref{lst:ant_system_complete}.

\begin{lstlisting}[language=Haskell,frame=tb,numbers=left,firstnumber=42,label=lst:ant_system_complete,caption=Transformation of a configuration for an ant system into a process hierarchy.]
interpret :: Conf -> Process Conf Path
interpret conf@Conf{..} = do
  let antProcs  = replicate ants antP
      innerProc = Multilel antProcs conf (Local combinePathsP)
      loop      = Loop conf 0 (<iter) id (\_ i -> i+1) innerProc
  Sequence loop extractSolutionP
\end{lstlisting}

Conceptually, the ant system we build into a \textsc{Hive} process does its work by running multiple ants in a loop and keeping track of the best solution they find. After a predefined maximum number of iterations, the loop terminates and the found solution is returned. Exactly this structure can be seen in the composition of \texttt{loop} and \texttt{extractSolutionP} using the \texttt{Sequence} combinator in last line of \lstref{lst:ant_system_complete}. \texttt{extractSolutionP} is a basic \textsc{Hive} process, \texttt{loop} is a composed process, defined locally in \texttt{interpret}. As \lstref{lst:ant_system_complete} shows, \texttt{loop} is a loop process, created using \texttt{Loop}, that contains an inner process, i.e. \texttt{innerProc}, which in turn is a composition using the \texttt{Multilel} combinator of basic ant processes \texttt{antP}.

To create a set of concurrently running ant processes for the inside of the loop, we simply create a list containing the desired number of processes that should run in parallel. Then we wrap them into a \texttt{Multilel} process together with an input value, i.e. \texttt{conf}, and a process to combine the solutions that will be found. Since the overhead for combining the solutions on a remote node would most likely be very high, we wrap the combinator process \texttt{combinePathsP} into a \texttt{Local} process so the process interpreter will carry out this computation locally. Note that instead of \texttt{Multilel}, we could also have used a hierarchy of \texttt{Parallel} processes to achieve the same semantics. The runtime however would be significantly worse due to the way the interpretation of \texttt{Parallel} processes has been implemented (cf. \lstref{lst:runprocess_parallel}). In a hierarchy of \texttt{Parallel} processes, we would have to wait for the processes furthest down in the hierarchy to terminate and run the combinator process multiple times on the way back up to the root, once for every \texttt{Parallel} node.

The loop is created by wrapping the process that describes the parallel execution of the ant processes using the \texttt{Loop} decorator and adding control information about when to terminate the loop. In this case, we want to terminate the loop after a predefined maximum number of iterations: we initialise a counter with value \texttt{0} and provide a function that increases this value after each iteration by $1$, i.e. \texttt{\textbackslash\_ i -> i+1}. Furthermore we need to provide a value that defines the result of the loop in case it is executed $0$ times, i.e. \texttt{conf}, and a function that transforms the result of the process inside the loop back into a form that can be used as the input value for the next iteration, i.e. \texttt{id}.