\chapter{Related Work}
We give an overview on related work that has been done in this field. First, we briefly introduce two process calculi, i.e. formalisms that allow to describe the behaviour of processes. After that, we discuss how these formal concepts can be used in programming.

\section{Process calculi}
Process calculi\index{Process!calculus} are formalisms that can be used to describe processes and their behaviour in a concurrent system on a high level. The description is done in a mathematical way, similar to algebraic structures\index{Algebraic!structure}, and independently from an implementation. Usually process calculi operate on abstract processes and provide a set of operators and combinators which can be used to compose processes. By introducing this kind of formalism, process calculi make it possible to reason about processes and perform (equivalence) transformations on them, e.g. to obtain a more optimised\footnote{Optimised according to a defined criterion.} representation. In \cite{Hoare:2012:LPU:2368298.2368301}, an algebraic model, which can be used to derive different process calculi, is discussed. The common, generalised model for various process calculi shows that these calculi are essentially equivalent and can be used to express the same thing.

\subsection{Calculus of Communicating Systems\index{Calculus of Communicating Systems}\index{CCS}}
The Calculus of Communicating Systems, short \textsc{CCS}, is a process algebra introduced by Robin Milner\index{Milner, Robin} in 1982 \cite{Milner:1982:CCS:539036}. \textsc{CCS} was one of the first process calculi in the history of computer science. It comprises a relatively small set of combinators\index{Combinator} which are used to describe process behaviour. We give an overview over most of them and illustrate how to use them. Note that our introduction to \textsc{CCS} is not complete and we only want to show the general idea.

In \textsc{CCS}, processes (also called \textit{agents}) have a name and are equipped with a set of labelled ports. A port can be seen as an endpoint of a communication channel and can be used for either sending or receiving messages, depending on which end they represent. For a channel with name $c$, the sending port is denoted by $c$ and the receiving port is denoted by $\overline{c}$. Processes can perform internal actions, e.g. a calculation, and external actions, i.e. sending or receiving values over ports. 

Processes are defined inductively: $\emptyset$ is the empty process that doesn't perform any action. Using the prefixing operator \enquote{.}, processes can be prefixed with an action. Let $P$ be a process and $a$ be an action, then $a.P$ is a process that performs action $a$ and then behaves like $P$. Repetition can be expressed using recursive process definitions: a process that repeatedly performs an action $a$ can be described by $P := a.P$.

Other operators for process composition are the choice operator \enquote{+} and the parallel operator \enquote{|}. Let $P, Q$ be processes. $P + Q$ creates a process that behaves like $P$ or like $Q$, where the choice between $P$ and $Q$ is made non-deterministically. $P \,|\, Q$ creates a process that executes $P$ and $Q$ at the same time.

Processes that are created using the parallel combinator can share channels and use them to send messages to each other. In \textsc{CCS}, communication over channel requires participation of processes at both endpoints at the same time. When a process sends a message over a channel, it must wait for the process on the other side to be ready to receive the message and vice versa. As a consequence, exchanging messages always includes mutual synchronisation of the participating processes. Let $a, b$ be actions, let $c$ be a channel and let $x$ be a value to be sent over $c$. Then two processes $P, Q$ that participate it the exchange of $x$ over $c$ may be described by $P := a.c\left(x\right).P$ and $Q := b.\overline{c}\left(x\right).Q$, where $P$ is in control over the sending port of $c$ and $Q$ is in control over the receiving port of $c$. The composition $P \,|\, Q$ enables them to use $c$ as a channel to synchronise and exchange information.

\subsection{Communicating Sequential Processes\index{Communicating Sequential Processes}\index{CSP}}
Communicating Sequential Processes, or short \textsc{CSP}, was developed by C.A.R. Hoare\index{Hoare, C.A.R.} and published in 1985 \cite{Hoare:1985:CSP:3921}. It arose around the same time as \textsc{CCS} and belongs to the circle of the first process calculi\index{Process!calculus}. We introduce a subset of \textsc{CSP} and show how to build processes using \textsc{CSP}, however we don't go too deep into detail, neither do we claim to provide a complete description.

In \textsc{CSP}, processes are composed of two types of primitives: sequential processes $\mathcal{P}$ and events $\mathcal{E}$. A process $P \in \mathcal{P}$ can observe an event\index{Event} $e \in \mathcal{E}$ and react to it. It cannot take influence on events or manipulate them as they are indivisible. In the following, let $P, Q \in \mathcal{P}$ be processes and $a,b \in \mathcal{E}$ be events.

The prefix combinator \enquote{$\to$} takes an event $a$ and a process $P$. $\left( a \to P \right)$ creates a new process that waits until it observes $a$ and then behaves like $P$. A process that repeatedly waits to observe $a$ and then behaves like $P$ can be described using recursion: $P = \left( a \to P \right)$.

With the choice operator \enquote{$|$} we can construct a process that allows two flows of control. The process $\left( a \to P \,|\, b \to Q \right)$ waits until it either observes $a$ and then behaves like $P$ or observes $b$ and then behaves like $Q$. Note that the prefix operator binds stronger than the choice operator and that the choice operator has commutative property.

The parallel operator \enquote{$||$} allows us to put processes together such that they run in parallel. The process obtained by $\left( P \,||\, Q \right)$ waits for an event that both $P$ and $Q$ can observe and then behaves like $P$ and $Q$ concurrently\footnote{Note there is a difference between concurrency and nondeterminism.}. Let $P = \left( a \to P \,|\, b \to a \to P \right)$ and $Q = \left( a \to Q \right)$ then $\left( P \,||\, Q \right) = \left( a \to P \,||\, a \to Q \right)$, since $Q$ cannot observe $b$ and thus the second choice from $P$ has to be omitted.

In contrast to the parallel operator \enquote{$||$}, the interleaving operator \enquote{$|||$} can be used to put processes together to execute concurrently, independently from which events they are able to observe. The process $\left( P \,|||\, Q \right)$ behaves like $P$ and $Q$ at the same time. If an event occurs that either of the processes can observe, the according process does so. If an event occurs that both processes, $P$ and $Q$, are able to observe, then the choice for which of them observes it is made nondeterministically.

Communication between processes is done by using channels: a channel exist between exactly two processes and is uni-directional. Let $c$ be a channel, $P$ be the process at the sending end of the channel and $Q$ be the process at the receiving end of the channel. Then $P$ can send a message $m$ over the channel using the sending operator \enquote{$!$}: $c!m$. $Q$ can receive this message using the receiving operator \enquote{$?$}: $c?m$. Sending and receiving messages are ordinary events, created and observed by processes. A process that reads messages from channel $c_{in}$ and forwards them to channel $c_{out}$ may look like this: $R = \left( c_{in}?m \to c_{out}!m \to R \right)$.

We have now seen some basic combinators of \textsc{CSP} and how to use them in order to create composed processes based on primitive processes and events. We can construct processes that run in parallel and make their execution dependant on observable events. The formalism of \textsc{CSP} allows reasoning about processes and transformations on them. \textsc{CSP} comes with many more operators and combinators than the ones we have introdcued, but as already mentioned we do not intend to give a complete description.


\section{Programming languages}
Most programming languages come with support for parallel and concurrent programming, either built into the language or available as some sort of extension. This makes it possible to transform the description of processes made in e.g. \textsc{CCS} or \textsc{CSP} into executable programs. This transformation is fairly straight-forward: sequential composition can be expressed by chaining together a sequence of processes, executing exactly one of them at a time in the order of composition. Parallel execution of processes can be done by executing each of the processes in a separate thread and waiting for all of them to terminate. The choice between processes can usually be expressed using \textit{if-then-else} compositions.

\subsection{Occam\index{Occam}}
The programming language \textsf{occam}\footnote{The reference manual for \textsf{occam} can be found at \url{http://www.wotug.org/occam/documentation/oc21refman.pdf}}, which first appeared in 1983, resembles the ideas of CSP closely. It incorporates direct implementations of the operations of choice between processes as well as parallel and sequential combination of processes. Furthermore it introduces named channels and both sending and receiving operations on them.

Sending data on a channel can be done using the sending operator \texttt{!}, receiving data from a channel can be done using the receiving operator \texttt{?}, see \lstref{lst:occam_send_receive}.

\begin{lstlisting}[language=Caml, caption=Sending data over a channel and receiving data from a channel in \textsf{occam}., label=lst:occam_send_receive, numbers=left, frame=bt]
channel ! out
channel ? in
\end{lstlisting}

In contrast to many other programming languages where sequential composition is implicit, \textsf{occam} has an operator for sequential composition of expressions: \texttt{SEQ}. It takes a list of processes and executes them sequentially as shown in \lstref{lst:occam_seq}.

\begin{lstlisting}[language=Caml, caption=Sequential composition of processes in \textsf{occam}., label=lst:occam_seq, numbers=left, frame=bt, firstnumber=3]
SEQ
  x := 3 * 7
  y := x * 2
\end{lstlisting}

Parallel composition can be done using the \texttt{PAR} combinator in \textsf{occam}. Like \texttt{SEQ}, it takes a list of processes and evaluates them concurrently as shown in \lstref{lst:occam_par}.

\begin{lstlisting}[language=Caml, caption=Parallel composition of processes in \textsf{occam}., label=lst:occam_par, numbers=left, frame=bt, firstnumber=6]
PAR
  x := f(a)
  y := g(b)
\end{lstlisting}

A non-deterministic\index{Non-determinism} choice between different alternatives of processes can be made using the \texttt{ALT} combinator. It takes a list of guarded processes and non-deterministically selects one of them for execution provided its guard signals readiness for the execution of the process. If none of the guards signals readiness, \texttt{ALT} waits until one of them does. A guard can be either a boolean expression, the action of reading from a channel or the combination of both.

\begin{lstlisting}[language=Caml, caption=Choice between process alternatives in \textsf{occam}., label=lst:occam_par, numbers=left, frame=bt, firstnumber=9]
ALT
  x > 3 & chan1 ? msg
    SEQ
      ...
  chan2 ? msg
    PAR
      ...
  ...
\end{lstlisting}

\clearpage
\subsection{Haskell\index{Haskell}}
In \textsf{Haskell}, there are a number of different modules available for expressing concurrency, parallelism\footnote{Note that there is a difference between concurrency and parallelism: while parallelism aims at performing a computation more quickly by executing computations on different processors, concurrency is a concept to structure programs and express actions that \textbf{conceptually} happen at the same time by executing them in different threads of control. However, it is an implementation detail if these threads actually run at the same time \cite{Marlow}.} and distribution. Typically, these modules introduce a monad that comes with different operations for the expression of computations, such as sequential or parallel execution. Two of these modules are \textsf{Concurrent Haskell}\index{Concurrent Haskell} in \texttt{Control.Concurrent} for concurrency and \textsf{Parallel Haskell}\index{Parallel Haskell} in \texttt{Control.Parallel} for parallelism. A third module for distribution, \textsf{Cloud Haskell}\index{Cloud Haskell} in \texttt{Control.Distributed.Process}, is discussed later in \chpref{chp:cloud_haskell}.

\subsubsection{Concurrent Haskell\index{Concurrent Haskell}}
\textsf{Concurrent Haskell} introduces the operation \texttt{forkIO} that creates a new lightweight thread that executes concurrently in the \texttt{IO} monad. Furthermore, a communication mechanism for threads is introduced: mutable variables, or \texttt{MVar}s. An \texttt{MVar} can either be full or empty and can be used by threads to communicate information. \texttt{MVars} can be read when they are full, information can be stored in them when they are empty. When a thread tries to read from an empty \texttt{MVar} or write to a full \texttt{MVar}, it is suspended until the \texttt{MVar} changes its state, caused by other threads executing according operations on them. This means, \texttt{MVar}s can be used for synchronisation.

Assume there are operations \texttt{a}, \texttt{b}, \texttt{c} where \texttt{a} and \texttt{b} take an \texttt{MVar} and can be executed concurrently. \texttt{c} consumes the results produced by \texttt{a} and \texttt{b} and returns a result. Using \texttt{Concurrent Haskell}, this can be expressed as shown in \lstref{lst:example_concurrent_haskell}.

\begin{lstlisting}[language=Haskell, numbers=left, frame=bt, label=lst:example_concurrent_haskell, caption={Parallel and sequential composition in \textsf{Concurrent Haskell}.}]
main :: IO ()
main = do
  -- create mvars for communication and synchronisation
  mvarA <- newEmptyMVar
  mvarB <- newEmptyMVar  
  
  -- run a and b in new threads
  _ <- forkIO (a mvarA)
  _ <- forkIO (b mvarB)  
  
  -- wait for a and b to return
  ra <- takeMVar mvarA
  rb <- takeMVar mvarB
  
  res <- c ra rb
  ...
\end{lstlisting}

\subsubsection{Parallel Haskell\index{Parallel Haskell}}
\textsf{Parallel Haskell} introduces the \texttt{Eval} monad along with the operations \texttt{rpar}, \texttt{rseq} and \texttt{runEval}. \texttt{rpar} is used to create a parallel computation while \texttt{rseq} is used to create a computation that is executed sequentially. \texttt{runEval} is used to execute computations created with \texttt{rpar} and \texttt{rseq}. In contrast to \textsf{Concurrent Haskell}, \texttt{Parallel Haskell} doesn't need the \texttt{IO} monad, the computations in the \texttt{Eval} monad are pure.

Analogously to before, assume there are function \texttt{a}, \texttt{b} and \texttt{c}. \texttt{a} and \texttt{b} can execute in parallel and \texttt{c} consumes the results produced by \texttt{a} and \texttt{b} but cannot start before both these values are available. Using \textsf{Parallel Haskell}, this can be expressed as shown in \lstref{lst:example_parallel_haskell}
\begin{lstlisting}[language=Haskell, numbers=left, frame=bt, label=lst:example_parallel_haskell, caption={Parallel and sequential composition in \textsf{Parallel Haskell}.}]
runEval $ do
  -- execute a and b in parallel
  ra <- rpar a
  rb <- rpar b
  
  -- wait for both a and b to return
  rseq ra
  rseq rb
  
  res <- c ra rb
  ...
\end{lstlisting}