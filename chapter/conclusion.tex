\chapter{Conclusion}
Let us recall the motivation and the goals we set in the introductory chapter. Many real world problems belong to the class of computationally hard problems and thus, require a high amount of time to be solved. A common approach is to exploit parallelism and distribution, using a multiplicity of processors or a network of computers to split up the problem and carry out the necessary computations faster. The code produced for this purpose tends to look similar even in different algorithms and involves low level technicalities like e.g. thread management, network communication and synchronisation.

It was our goal to design and implement a process calculus that allows the developer to focus on the formulation of algorithms and express parallelism on a higher level of abstraction. The developer should not be bothered with thread management or explicit communication between processes. Instead, he should create a set of basic processes and put them together using process combinators to obtain the desired algorithm. The technical details of parallelisation should not be his concern, but be taken care of by an interpreter that interprets the process structure created by using the process combinators.

We have taken a look at two prominent process calculi\index{Process calculus}, namely \textsc{CCS} and \textsc{CSP} in \chpref{chp:process_calculi} and seen they are incompatible with our goals: they both involve explicit communication between processes and non-determinism\index{Non-determinism}. Explicit communication between processes is what we aim to hide and non-determinisitic behaviour is an undesired property in practical applications

We have then defined our own process calculus, in some points similar to \textsc{CCS} and \textsc{CSP}, but different in the points where incompatible with our goals. The developed process calculus is free of non-determinism and hides technical details. \textsf{Haskell} has been used to make a direct implementation of the calculus and reflect the defined properties for process composition. \textsf{Haskell}'s strong type system has been a valuable help in assuring correctness of constructed process terms.

In the implementation we have made use of \textsf{Concurrent Haskell} and \textsf{Cloud Haskell} with the idea to make both implementations usable interchangeably and either achieve parallelisation on one computer of distribution in a network of computers. However, at the moment full interchangeability can not be accomplished since the \textsf{Haskell} compiler \textsf{ghc} lacks some necessary features. We are optimistic that in the future interchangeability can be achieved.

To illustrate the functionality of the developed process calculus, we have implemented a meta-heuristic to solve the \textsc{TSP}: an artificial ant system. The implementation shows that the goals we have set earlier are reached: process composition can be expressed on a high level and does not involve technical details about parallelisation or process communication. Furthermore, the programs used for the test runs to asses the achievable speedup show that both implementations, the one based on \textsf{Concurrent Haskell} and the one based on \textsf{Cloud Haskell}, can actually be used almost interchangeably and require only little adjustments to the code.

The conducted performance tests have shown that employing our process calculus to create processes and using the implemented process interpreter to execute these processes introduces a noticeable management overhead. However, given a multitude of physical cores to carry out the computations, the implementation based on \textsf{Concurrent Haskell} has proven to be able to achieve a speedup compared to a non-parallelised variant of the same program. The same is not the case for the implementation based on \textsf{Cloud Haskell}: as the tests have shown, a speedup can't be achieved, not even with a high number of worker nodes connected. The investigation of this problem is left open for future work.

To summarise, we can say that the set goals have been met. The formulation of parallel programs on a high level of abstraction is elegantly possible using the developed process calculus. In addition to that, it is generally possible to achieve a speedup when using our process calculus to parallelise programs.