\chapter{Introduction\index{Introduction}}
In the introduction we start off with a motivation: what are the reasons for what we are going to do? After that we set our goals and give a brief overview about the achieved results.

\section{Motivation}
Both in theory and practice computer scientists encounter computationally hard problems in a large variety of fields. These include e.g. register allocation in compilers, timetable layout and tour planning, all of which are formalised in a graph theoretic\index{Graph theory} \cite{Garey:1979:CIG:578533} representation.

The limiting factor in these problems is the huge number of possible solutions, which \textbf{all} have to be examined in order to find the optimal solution \cite{Garey:1979:CIG:578533}. Even for small problem instances the number of possible solutions is so big that this approach is rendered impractical. To tackle this difficulty, usually approximation algorithms, parallel algorithms, distributed algorithms or a combination of these is employed. Approximation algorithms speed up finding solutions by only searching for close to optimal solutions, parallel and distributed algorithms break problems down into smaller pieces which then are solved concurrently.

Many of the currently widely used programming languages such as Java and Python come with libraries for development of parallel and distributed programs. These libraries contain tools for running code remotely, serialising data, sending data to remote processes as well as concepts for synchronisation. However, unrelated to the particular problem, the code produced in this context always looks very similar and involves rather technical details concerning communication between processes. Its re-usability is limited in that the interplay of communication and operations performed on the communicated data is inherently specific to the implemented algorithm. A similarly looking, because conceptually the same, yet different composition of the same code fragments is produced for different algorithms. The ability to describe communication patterns and data flow on a higher level of abstraction is both desirable and would allow developers to focus more on solving the actual problem than on the communication between processes.

\section{Goal}
\label{chp:goal}
Our goal is to design a toolbox that allows for the development of parallel and distributed programs on a higher level of abstraction. Ideally we want to be able to define computations, to compose and to execute them. This could happen locally on one computer as well as in a distributed system. No matter which option is chosen, we don't want the user of this toolbox to be concerned about communication between processes, serialisation of data or synchronisation. It should only be apparent \textbf{that} parallelism is introduced, not \textbf{how} this is done. Other than by modelling computations to be carried out in parallel, the involvement of multiple threads, processes or a distributed system should be hidden entirely from the developer.

We aim for an algebra on computations as easy to use as arithmetic operations on natural numbers. The algebra should only allow composition of compatible computations, i.e. only if their types match, respectively to the used combinator. We want to be able to check the validity of a described computation already at compile time to eliminate runtime errors that would arise from mismatching types.

The technique we use to achieve an \enquote{automated} distribution of a process structure is the interpreter\index{Interpreter} pattern \cite{Gamma:1995:DPE:186897}. The developer has to identify the pieces of an program and use our algebra to wrap these pieces into basic computations\index{Computation!basic}. Then he has to combine these basic computations with our provided combinators. When the interpreter\index{Interpreter} interprets a computation, it examines its structure and takes care of its execution, \enquote{automatically} parallelising the problem wherever modelled accordingly. This leaves the developer with the responsibility to identify and mark parallel pieces of the program. Note however, that we are \textbf{not} providing a tool that can parallelise any given algorithm automatically, since this is impossible due to the undecidability\footnote{The decidability of function equivalence would imply the decidability of the halting problem, which has been shown to be undecidable \citep{Garey:1979:CIG:578533}.} of function equivalence.

Once we have an algebra for computations, we want to assess its applicability in practice. We are interested in whether it is possible to achieve a speedup in execution of a program simply by modelling parts of it for parallel execution, both in case of parallelism on only one computer and in a distributed system.

\section{Results}
\lipsum[1-6]