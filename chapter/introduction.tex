\chapter{Introduction\index{Introduction}}
\vspace*{-0.5em}
In the introduction we start off with a motivation: what are the reasons for what we are doing? After that we set our goals and give a brief overview about our results.

\vspace*{-0.5em}
\section{Motivation}
Both in theory and practice, computer scientists encounter computationally hard problems in a large variety of fields. This includes, e.g. register allocation in compilers, timetable layout and tour planning, all of which are formalised in a graph theoretic\index{Graph theory} \cite{Garey:1979:CIG:578533} representation.

The limiting factor in these problems is the huge number of possible solutions, which \textbf{all} have to be examined in order to find the optimal solution \cite{Garey:1979:CIG:578533}. Even for small problem instances, the number of possible solutions is so big that this approach is rendered impractical. To tackle this difficulty, usually approximation algorithms, parallel algorithms, distributed algorithms or a combination of these is employed. Approximation algorithms speed up finding solutions by searching for approximate solutions, as their name suggests. Parallel and distributed algorithms break problems down into smaller pieces which then are solved in parallel.

Many of the currently widely used programming languages, such as \textsf{Haskell}, \textsf{Java} and \textsf{Python}, come with libraries for development of parallel and distributed programs. These libraries contain tools for running code remotely, serialising data, sending data to remote processes as well as concepts for synchronisation. Some languages, e.g. \textsf{Erlang} and \textsf{elixir}, are specially designed for the development of distributed software. However, unrelated to the particular problem, the code produced in this context always looks very similar and involves rather technical details concerning communication between processes. Its re-usability is limited insofar that code for process communication and synchronisation is mixed with the actual algorithm. A toolbox that hides these technical details and thereby makes it possible to describe algorithms on a higher level of abstraction is both desirable and would allow the developer to focus more on solving the actual problem than on the communication between processes.

\section{Goal}
\label{chp:goal}
Our goal is to design a toolbox that allows for the development of parallel and distributed programs on a higher level of abstraction. Ideally, we want to be able to define computations, to compose and to execute them. This could happen locally on one computer as well as in a distributed system. No matter which option is chosen, we do not want the user of this toolbox to be concerned about communication between processes, serialisation of data or synchronisation. It should only be apparent \textbf{that} parallelism is introduced, not \textbf{how} this is done. Other than by modelling computations to be carried out in parallel, the involvement of multiple threads, processes or a distributed system should be hidden entirely from the user.

We aim for a calculus on processes as easy to use as arithmetic operations on natural numbers. The calculus should only allow composition of compatible computations, i.e. only if their types match, respectively to the used combinator. We want to be able to check the validity of a described computation already at compile time to eliminate runtime errors that would arise from mismatching types.

The technique we use to achieve an \enquote{automated} distribution of a process structure is the interpreter\index{Interpreter} pattern \cite{Gamma:1995:DPE:186897}. The developer has to identify the pieces of a program and formulate them in a form of basic processes\index{Process!Basic} our calculus can operate on. Then he has to combine these basic computations with the process combinators provided by our calculus. When the interpreter\index{Interpreter} interprets a process, it examines its structure and takes care of its execution, \enquote{automatically} parallelising the problem wherever modelled accordingly. This leaves the developer with the responsibility to identify and mark parallel pieces of the program. Note, however, that we are \textbf{not} providing a tool that can parallelise any given algorithm automatically, since this is impossible due to the undecidability\footnote{The decidability of function equivalence would imply the decidability of the halting problem, which has been shown to be undecidable \citep{Garey:1979:CIG:578533}.} of function equivalence.

Once we have developed the calculus and made an implementation, we want to assess its applicability in practice. We are interested in whether it is possible to achieve a speedup in execution of a program simply by modelling parts of it for parallel execution and using a suitable interpreter, both in case of parallelism on only one computer and in a distributed system.

\section{Results}
We introduce a process calculus that can be used to describe parallel programs on a high level of abstraction and use \textsf{Haskell} for the implementation. The process calculus hides technical details concerning process management and communication between processes. Tests show that it is generally possible to achieve a speedup by parallelising programs using our process calculus. However, in the case of distribution of processes across a network of computers, we were not able to achieve a speedup so far and future work remains to be done.