\chapter{Introduction\index{Introduction}}
In the introduction we will start off with a motivation: what are the reasons for what we are going to do? After that we will set our goals and give a brief overview about the achieved results.

\section{Motivation}
Both in theory and practice computer scientists encounter computationally hard problems in a large variety of fields. These include e.g. register allocation in compilers, timetable layout and tour planning, all of which are formalised in a graph theoretic\index{Graph theory} \cite{Garey:1979:CIG:578533} representation.

The limiting factor in these problems is the huge number of possible solutions, which \textbf{all} have to be examined in order to find the optimal solution \cite{Garey:1979:CIG:578533}. Even for small problem instances the number of possible solutions is so big that this approach is rendered impractical. To tackle this difficulty, usually approximation algorithms \cite{}, distributed algorithms or distributed approximation algorithms are employed. The idea behind distributed algorithms is to apply more processing power to find a solution faster. Approximation algorithms try to find a solution that is good enough to satisfy a defined criterion.

In contrast to \enquote{big data} where there is usually a data volume of several terabytes involved, we will only deal with problem instances of up to a few hundred KB\footnote{Depending on the chosen representation, the size might vary.}. Due to the huge amount of computations to be carried out and to make a distinction against \enquote{big data}, we refer to this as \enquote{big computation}.

Many of the currently widely used programming languages such as Java and Python come with libraries for development of distributed programs. These libraries contain tools for running code remotely, serialising\footnote{In fact, serialisation is usually done implicitly by the runtime system.} data, sending data to remote processes as well as concepts for synchronisation. These tools are exactly we use use to build distributed algorithms. However, unrelated to the particular problem, the code produced in this context always looks very similar and doesn't come with further enlightenment.

A bit more precisely, we are dealing with intractable problems from the class $\mathcal{NPC}$\footnote{$\mathcal{NPO}$ respectively in the case of optimisation problems.} \cite{Garey:1979:CIG:578533}, \cite{Hopcroft:2006:IAT:1196416}. For these problems it takes up a huge amount of time to search the solution space since it is exponential in the size of the problem input. It is widely believed, though not proven, that these problems cannot be solved to optimality in polynomial time. In fact it isn't even known if this property can be formally proven at all \cite{}. Nevertheless many research results suggest that this \enquote{intuition} holds \cite{} \cite{} \cite{}.

\section{Goal}
\label{chp:goal}
Our goal is to design a toolbox that allows for the development of distributed algorithms on a higher level of abstraction. Ideally we want to be able to take functions, wrap them into processes, compose these processes and run the resulting process in a distributed system. We don't want the user of this toolbox to be concerned about communication between processes, serialisation of data or synchronisation of processes. The fact that the process will be executed in a disctributed system should be hidden entirely.

We aim for an algebra on processes as easy to use as arithmetic operations on natural numbers. The algebra should only allow composition of compatible processes, i.e. only if their types match respectively to the used combinator. The reason for this is to eliminate runtime errors that arise from mismatching types.

Although we are going to hide the fact that the constructed processes are executed in a distributed system, we want to achieve a speedup in execution. Compared to being run on a single computer, execution in a distributed system, involving potentially hundreds or thousands of computers, should yield a significant speedup.

The technique we use to achieve an \enquote{automated} distribution of a process structure is the interpreter\index{Interpreter} pattern \cite{Gamma:1995:DPE:186897}. The user has to identify the pieces of an algorithm and use our process algebra to wrap these pieces into basic processes\index{Process!basic}. Then he has to combine these basic processes with our provided process combinators. When the interpreter\index{Interpreter} interprets such a process, it will examine its structure and distribute the parallel parts accordingly. However, this leaves the user with the responsibility to identify and mark parallel pieces of the algorithm. Note that we are \textbf{not} providing a tool that can parallelise any given algorithm automatically since this is impossible due to the undecidability of function equivalence stated by \textsc{Rice}'s theorem \cite{}.

\section{Results}
\lipsum[1-6]